{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "import nltk\n",
    "import string\n",
    "import os\n",
    "import torch\n",
    "from collections import Counter\n",
    "from transformers import AutoModel, AutoTokenizer, BertTokenizer\n",
    "from gensim.models import KeyedVectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Carrega a BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_tokenizer = AutoTokenizer.from_pretrained('/root/.cache/torch/transformers/neuralmind-bert-large-portuguese-cased/')\n",
    "bert_model = AutoModel.from_pretrained('/root/.cache/torch/transformers/neuralmind-bert-large-portuguese-cased/')\n",
    "\n",
    "# bertTokenizer = AutoTokenizer.from_pretrained('neuralmind/bert-large-portuguese-cased')\n",
    "# bertModel = AutoModel.from_pretrained('neuralmind/bert-large-portuguese-cased')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Carrega os dados de entrada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/opt/program/projects/relation-extraction-deep-learning/data\n"
     ]
    }
   ],
   "source": [
    "pwd = %pwd\n",
    "pwd = os.path.join(os.path.dirname(pwd), 'data')\n",
    "print(pwd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(os.path.join(pwd, 'input/data_full.csv'), sep=';').replace({np.NaN: None})\n",
    "data.columns = ['sentenca','entidade1','entidade1_tipo','relacao','entidade2','entidade2_tipo']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['class'] = data['relacao'].apply(lambda x: 0 if x is None else 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentenca</th>\n",
       "      <th>entidade1</th>\n",
       "      <th>entidade1_tipo</th>\n",
       "      <th>relacao</th>\n",
       "      <th>entidade2</th>\n",
       "      <th>entidade2_tipo</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A Kroton anunciou uma parceria com o Cubo Itaú...</td>\n",
       "      <td>Kroton</td>\n",
       "      <td>ORG</td>\n",
       "      <td>anunciou uma parceria com</td>\n",
       "      <td>Cubo Itaú</td>\n",
       "      <td>ORG</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>O evento Summit AgriHub reuniu nesta quinta-fe...</td>\n",
       "      <td>Summit AgriHub</td>\n",
       "      <td>ORG</td>\n",
       "      <td>reuniu em</td>\n",
       "      <td>Cuiabá</td>\n",
       "      <td>PLC</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>O evento Summit AgriHub reuniu nesta quinta-fe...</td>\n",
       "      <td>Summit AgriHub</td>\n",
       "      <td>ORG</td>\n",
       "      <td>None</td>\n",
       "      <td>Mato Grosso</td>\n",
       "      <td>PLC</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>O evento Summit AgriHub reuniu nesta quinta-fe...</td>\n",
       "      <td>Summit AgriHub</td>\n",
       "      <td>ORG</td>\n",
       "      <td>None</td>\n",
       "      <td>Brasil</td>\n",
       "      <td>PLC</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Ousuário consegue comparar prazos , condições ...</td>\n",
       "      <td>Bradesco</td>\n",
       "      <td>ORG</td>\n",
       "      <td>None</td>\n",
       "      <td>Banco do Brasil</td>\n",
       "      <td>ORG</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            sentenca       entidade1  \\\n",
       "0  A Kroton anunciou uma parceria com o Cubo Itaú...          Kroton   \n",
       "1  O evento Summit AgriHub reuniu nesta quinta-fe...  Summit AgriHub   \n",
       "2  O evento Summit AgriHub reuniu nesta quinta-fe...  Summit AgriHub   \n",
       "3  O evento Summit AgriHub reuniu nesta quinta-fe...  Summit AgriHub   \n",
       "4  Ousuário consegue comparar prazos , condições ...        Bradesco   \n",
       "\n",
       "  entidade1_tipo                    relacao        entidade2 entidade2_tipo  \\\n",
       "0            ORG  anunciou uma parceria com        Cubo Itaú            ORG   \n",
       "1            ORG                  reuniu em           Cuiabá            PLC   \n",
       "2            ORG                       None      Mato Grosso            PLC   \n",
       "3            ORG                       None           Brasil            PLC   \n",
       "4            ORG                       None  Banco do Brasil            ORG   \n",
       "\n",
       "   class  \n",
       "0      1  \n",
       "1      1  \n",
       "2      0  \n",
       "3      0  \n",
       "4      0  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Essa sera nossa estrategia de encoding para propagar esses dados pela rede neural"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "O evento Summit AgriHub reuniu nesta quinta-fe...\t -> [0,0,0...,0]\n",
    "Summit AgriHub\t -> [0,0,0...,0]\n",
    "Cuiabá\t -> [0,0,0...,0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verifica em quantos registros existem relacoes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({1: 463, 0: 446})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter(data['class'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos pegar o tamanho maximo das sentencas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len = data['sentenca'].apply(lambda x: len(nltk.word_tokenize(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    909.000000\n",
       "mean      40.207921\n",
       "std       15.261946\n",
       "min        8.000000\n",
       "25%       30.000000\n",
       "50%       39.000000\n",
       "75%       48.000000\n",
       "max      128.000000\n",
       "Name: sentenca, dtype: float64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.Series(max_len).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len = max(max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "128"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BERT tokens: ['A', 'K', '##ro', '##ton', 'anunciou', 'uma', 'parceria', 'com', 'o', 'Cub', '##o', 'Ita', '##ú', ',', 'que', 'dar', '##á', 'origem', 'à', 'vertical', '“', 'Cub', '##o', 'Educa', '##tion', '”', ',', 'um', 'espaço', 'dedicado', 'ao', 'fome', '##nto', 'de', 'ed', '##tec', '##h', '##s', '–', 'tecnologia', 'aplicada', 'à', 'educação', '.']\n",
      "NLTK tokens: ['A', 'Kroton', 'anunciou', 'uma', 'parceria', 'com', 'o', 'Cubo', 'Itaú', ',', 'que', 'dará', 'origem', 'à', 'vertical', '“', 'Cubo', 'Education', '”', ',', 'um', 'espaço', 'dedicado', 'ao', 'fomento', 'de', 'edtechs', '–', 'tecnologia', 'aplicada', 'à', 'educação', '.']\n"
     ]
    }
   ],
   "source": [
    "sample_txt = 'A Kroton anunciou uma parceria com o Cubo Itaú , que dará origem à vertical “Cubo Education” , um espaço dedicado ao fomento de edtechs – tecnologia aplicada à educação .'\n",
    "print(f\"BERT tokens: {bert_tokenizer.tokenize(sample_txt)}\")\n",
    "print(f\"NLTK tokens: {nltk.word_tokenize(sample_txt)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Sentence: A Kroton anunciou uma parceria com o Cubo Itaú , que dará origem à vertical “Cubo Education” , um espaço dedicado ao fomento de edtechs – tecnologia aplicada à educação .\n",
      "   Tokens: ['A', 'Kroton', 'anunciou', 'uma', 'parceria', 'com', 'o', 'Cubo', 'Itaú', ',', 'que', 'dará', 'origem', 'à', 'vertical', '“', 'Cubo', 'Education', '”', ',', 'um', 'espaço', 'dedicado', 'ao', 'fomento', 'de', 'edtechs', '–', 'tecnologia', 'aplicada', 'à', 'educação', '.']\n",
      "Token IDs: [177, 100, 3127, 230, 4495, 170, 146, 100, 100, 117, 179, 100, 2008, 353, 14357, 1112, 100, 100, 22354, 117, 222, 2363, 8055, 320, 100, 125, 100, 1379, 4277, 11107, 353, 3478, 119, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "WE: {'input_ids': [177, 461, 157, 897, 3127, 230, 4495, 170, 146, 18842, 22280, 4008, 22332, 117, 179, 2822, 22303, 2008, 353, 14357, 1112, 18842, 22280, 14191, 1131, 22354, 117, 222, 2363, 8055, 320, 11062, 234, 125, 902, 8948, 22296, 22281, 1379, 4277, 11107, 353, 3478, 119, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}\n"
     ]
    }
   ],
   "source": [
    "tokens = nltk.word_tokenize(sample_txt)\n",
    "token_ids = bert_tokenizer.convert_tokens_to_ids(tokens)\n",
    "token_ids = token_ids + [0]*(max_len-len(token_ids))\n",
    "\n",
    "print(f' Sentence: {sample_txt}')\n",
    "print(f'   Tokens: {tokens}')\n",
    "print(f'Token IDs: {token_ids}')\n",
    "\n",
    "we = bert_tokenizer.encode_plus(\n",
    "  sample_txt,\n",
    "  max_length=max_len,\n",
    "  add_special_tokens=False, # Add '[CLS]' and '[SEP]'\n",
    "  return_token_type_ids=False,\n",
    "  pad_to_max_length=True,\n",
    "  return_attention_mask=False,\n",
    "    truncation=True\n",
    ")\n",
    "print(f'WE: {we}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos entao passar todas as frases para lista de tokens_id <br>\n",
    "Uma maneira legal de fazer isso é usando namedtuples, assim conseguimos colocar varios dados dentro de uma mesma tupla e depois fazer o parse deles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import namedtuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "Row = namedtuple(\"Row\", [\"sentenca\",\"entidade1\",\"entidade2\",\"relacao\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = data.apply(\n",
    "    lambda x: Row(\n",
    "        sentenca=pad(input_list=bert_tokenizer.convert_tokens_to_ids(nltk.word_tokenize(x['sentenca'])), max_size=max_len),\n",
    "        entidade1=pad(input_list=bert_tokenizer.convert_tokens_to_ids(nltk.word_tokenize(x['entidade1'])), max_size=max_len),\n",
    "        entidade2=pad(input_list=bert_tokenizer.convert_tokens_to_ids(nltk.word_tokenize(x['entidade2'])), max_size=max_len),\n",
    "        relacao= None if x['relacao'] is None else pad(input_list=bert_tokenizer.convert_tokens_to_ids(nltk.word_tokenize(x['relacao'])), max_size=max_len)\n",
    "    )\n",
    "    , axis=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(sentenca=[177, 100, 3127, 230, 4495, 170, 146, 100, 100, 117, 179, 100, 2008, 353, 14357, 1112, 100, 100, 22354, 117, 222, 2363, 8055, 320, 100, 125, 100, 1379, 4277, 11107, 353, 3478, 119, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], entidade1=[100, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], entidade2=[100, 100, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], relacao=[3127, 230, 4495, 170, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agora vamos encodar esses ids em vetores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 1\n"
     ]
    }
   ],
   "source": [
    "print_every = 10\n",
    "for i in range(len(corpus)):\n",
    "    \n",
    "    corpus[i] = Row(\n",
    "        sentenca=bert_model(torch.tensor(corpus[i].sentenca).unsqueeze(0))[0],\n",
    "        entidade1=bert_model(torch.tensor(corpus[i].entidade1).unsqueeze(0))[0],\n",
    "        entidade2=bert_model(torch.tensor(corpus[i].entidade2).unsqueeze(0))[0],\n",
    "        relacao= None if corpus[i].relacao is None else bert_model(torch.tensor(corpus[i].relacao).unsqueeze(0))[0]\n",
    "    )\n",
    "    \n",
    "    if i % print_every == 0:\n",
    "        print(f'Iteration: {i+1}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Elaboracao da rede neural"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos fazer com que a rede tenha apenas um ou mais modelos internos. Por exemplo, um modelo para verificar se existe relacao e outro para extrair essa relacao"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "free_text -> Abordagem |\n",
    "                        -> Verificacao se existe relacao |-> (sim) -> Extracao da relacao\n",
    "                                                         |-> (nao) -> Informa que nao tem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "É possivel receber modelos por parametros dentro das classes de nn.Module. Assim, incorporamos o modelo recebido as configuracao da rede que estamos propondo. <br>\n",
    "Abaixo é possivel ver que incorporamos a BERT nos modelos ModeloVerificaRelacao e ModeloExtraiRelacao. Alem disso, incorporamos esses dois modelos tambem em ModeloDaniel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deste modo, temos as seguintes possibilidades:<br>\n",
    "    - Os modelos ModeloVerificaRelacao e ModeloExtraiRelacao podem propagar suas entradas pela rede da BERT<br>\n",
    "    - Treinar apenas o ModeloDaniel e fazer com que ele propague o erro para ModeloVerificaRelacao, ModeloExtraiRelacao e BERT (Isso é considerado um fine-tuning da BERT; O desafio sera propagar as saidas de ModeloVerificaRelacao para ModeloExtraiRelacao)<br>\n",
    "    - Treinar ModeloVerificaRelacao e ModeloExtraiRelacao separados e depois adicionalos ao ModeloDaniel apenas para unificacao (Essa abordagem é mais didatica; O desafio sera elaborar um metodo de treinamento consistente que permita apenas um treinamento, caso contrario deverao existir varios em separado e apenas um local para unificar suas chamadas)<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import TensorDataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepRelationIdentifier(nn.Module):\n",
    "    def __init__(self, embedding_model, max_sentence_length, features=3, embedding_size=1024, output_size=1, n_hidden=1, drop_prob=0.1):\n",
    "        super().__init__()\n",
    "        self._embedding_model = embedding_model\n",
    "        self._max_len = max_sentence_length\n",
    "        \n",
    "        self._drop_prob = drop_prob\n",
    "        self._n_hidden = n_hidden\n",
    "        \n",
    "        # Avoid tuning BERT to this problem\n",
    "        self._embedding_model.requires_grad = False\n",
    "        \n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(embedding_size * features, self._n_hidden),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(self._n_hidden, self._n_hidden),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(self._drop_prob),\n",
    "            nn.Linear(self._n_hidden, output_size),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "    def _pad(self, x):\n",
    "        return x + [0]*(self._max_len-len(x))   \n",
    "    \n",
    "    def parse_sentence_to_ids(self, sentence, pad=True):\n",
    "        sentence = bert_tokenizer.convert_tokens_to_ids(nltk.word_tokenize(sentence))\n",
    "        return self._pad(x=sentence) if pad else sentence\n",
    "    \n",
    "    def to_embeddings(self, x, is_token_ids=True):\n",
    "        if is_token_ids:\n",
    "            return self._embedding_model(torch.tensor(x).unsqueeze(0))[0]\n",
    "        else:\n",
    "            return self._embedding_model(torch.tensor(self.parse_sentence_to_ids(sentence=x, pad=True)).unsqueeze(0))[0]\n",
    "\n",
    "    def prepare_sentence(self, sentence:str, entity1:str, entity2:str):\n",
    "        features = (self.to_embeddings(sentence, is_token_ids=False) for f in [sentence, entity1, entity2])\n",
    "        return torch.cat(tuple(f.squeeze().sum(axis=0) for f in features), dim=0).cpu().detach().numpy()\n",
    "                    \n",
    "    def forward(self, inputs):\n",
    "        ''' Forward pass through the network'''\n",
    "        ## TODO: put x through the fully-connected layer\n",
    "        out = self.fc(inputs)\n",
    "        \n",
    "        # return the final output and the hidden state\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [],
   "source": [
    "ri_model = DeepRelationIdentifier(\n",
    "    embedding_model=bert_model,\n",
    "    max_sentence_length=max_len,\n",
    "    features=3,\n",
    "    embedding_size=1024,\n",
    "    output_size=1,\n",
    "    n_hidden=1,\n",
    "    drop_prob=0.1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imprime o schema do modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(ri)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(net, data, epochs=10, batch_size=10, lr=0.001, clip=5, val_frac=0.1, print_every=10, earlyStopping=False, max_epochs_no_improve=2):\n",
    "    '''\n",
    "        Training a network \n",
    "    '''\n",
    "    net.train()\n",
    "    \n",
    "    opt = torch.optim.SGD(net.parameters(), lr=lr)\n",
    "    criterion = nn.BCELoss()\n",
    "    \n",
    "    # create training and validation data\n",
    "    valid_idx = int(len(data)*(1-val_frac))\n",
    "    data, valid_data = data[:valid_idx], data[valid_idx:]\n",
    "    \n",
    "    # create Tensor datasets\n",
    "    train_data = TensorDataset(torch.from_numpy(np.array([x[0] for x in data])), torch.from_numpy(np.array([x[1] for x in data])))\n",
    "    valid_data = TensorDataset(torch.from_numpy(np.array([x[0] for x in valid_data])), torch.from_numpy(np.array([x[1] for x in valid_data])))\n",
    "\n",
    "    # shuffling and batching data\n",
    "    train_loader = DataLoader(train_data, shuffle=True, batch_size=batch_size)\n",
    "    valid_loader = DataLoader(valid_data, shuffle=True, batch_size=batch_size)\n",
    "    \n",
    "    if(torch.cuda.is_available()):  \n",
    "        net.cuda()\n",
    "    \n",
    "    epoch_no_improve = 0\n",
    "    last_epoch_loss = None\n",
    "    \n",
    "    for e in range(epochs):\n",
    "        counter = 0\n",
    "\n",
    "        losses = []\n",
    "        for inputs, targets in train_loader:\n",
    "            counter += 1\n",
    "\n",
    "            if(torch.cuda.is_available()):\n",
    "                inputs, targets = inputs.cuda(), targets.cuda()\n",
    "\n",
    "            # zero accumulated gradients\n",
    "            net.zero_grad()\n",
    "            \n",
    "            # get the output from the model\n",
    "            output = net(inputs)\n",
    "            # calculate the loss and perform backprop\n",
    "            loss = criterion(output.float(), targets.float())\n",
    "               \n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "\n",
    "            losses.append(loss.item())\n",
    "            \n",
    "            # loss stats\n",
    "            if counter % print_every == 0:\n",
    "                # Get validation loss\n",
    "                val_losses = []\n",
    "                net.eval()\n",
    "                \n",
    "                for inputs, targets in valid_loader:\n",
    "\n",
    "                    if(torch.cuda.is_available()):\n",
    "                        inputs, targets = inputs.cuda(), targets.cuda()\n",
    "\n",
    "                    output = net(inputs)\n",
    "                    val_loss = criterion(output.float(), targets.float())\n",
    "\n",
    "                    val_losses.append(val_loss.item())\n",
    "                \n",
    "                net.train() # reset to train mode after iterationg through validation data\n",
    "\n",
    "                _loss = loss.item()\n",
    "                _val_loss = 1 if len(val_losses) == 0 else np.nanmean(val_losses)\n",
    "                \n",
    "                print('{0}{1}{2}{3}'.format(\n",
    "                    \"Epoch: {}/{}...\".format(e+1, epochs),\n",
    "                    \"Step: {}...\".format(counter),\n",
    "                    \"Loss: {:.4f}...\".format(_loss),\n",
    "                    \"Val Loss: {:.4f}\".format(_val_loss)\n",
    "                    )\n",
    "                )\n",
    "                print(\"loss:{:0.4f}\".format(_loss))\n",
    "                print(\"val_loss:{:0.4f}\".format(_val_loss))\n",
    "\n",
    "\n",
    "        # Early stopping strategy\n",
    "        current_epoch_loss = 1 if len(losses) == 0 else np.nanmean(losses)\n",
    "        if last_epoch_loss is None:\n",
    "            last_epoch_loss = current_epoch_loss\n",
    "        elif last_epoch_loss < current_epoch_loss:\n",
    "            epoch_no_improve += 1\n",
    "        else:\n",
    "            last_epoch_loss = current_epoch_loss\n",
    "            epoch_no_improve = 0\n",
    "\n",
    "        if earlyStopping:\n",
    "            if epoch_no_improve >= max_epochs_no_improve:\n",
    "                print('[EarlyStopping] Reached max epochs without improvement')\n",
    "                print(\"early_stopping:1\")\n",
    "                break\n",
    "            else:\n",
    "                continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import namedtuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "Row = namedtuple(\"Row\", [\"sentenca\",\"entidade1\",\"entidade2\",\"relacao\"])\n",
    "corpus = data.apply(\n",
    "    lambda x: Row(\n",
    "        sentenca=x['sentenca'],\n",
    "        entidade1=x['entidade1'],\n",
    "        entidade2=x['entidade2'],\n",
    "        relacao= None if x['relacao'] is None else x['relacao']\n",
    "    )\n",
    "    , axis=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = [\n",
    "    (\n",
    "        ri_model.prepare_sentence(sentence=x.sentenca, entity1=x.entidade1, entity2=x.entidade2).cpu().detach().numpy(), \n",
    "        0 if x.relacao is None else 1\n",
    "    ) for x in corpus[:5]\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = [(x[0].cpu().detach().numpy(), x[1]) for x in dataset]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_ration = 0.2\n",
    "test_idx = int(len(dataset)*(1-test_ration))\n",
    "trainset, testset = dataset[:test_idx], dataset[test_idx:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainset size: 4\n",
      "Testset size: 1\n"
     ]
    }
   ],
   "source": [
    "print(f\"Trainset size: {len(trainset)}\")\n",
    "print(f\"Testset size: {len(testset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {},
   "outputs": [],
   "source": [
    "train(\n",
    "    net=ri_model,\n",
    "    data=trainset,\n",
    "    epochs=2,\n",
    "    batch_size=10,\n",
    "    lr=0.001,\n",
    "    clip=5,\n",
    "    val_frac=0.1, \n",
    "    print_every=10,\n",
    "    earlyStopping=False,\n",
    "    max_epochs_no_improve=2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.metrics as metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_binary_classification(y_true, y_pred, pos_label=1):\n",
    "    \n",
    "    def metric_auc(y_true, y_pred, pos_label=True):\n",
    "        try:\n",
    "            fpr, tpr, _threshold = metrics.roc_curve(\n",
    "                y_true,\n",
    "                y_pred,\n",
    "                pos_label=pos_label\n",
    "            )\n",
    "            return metrics.auc(fpr, tpr)\n",
    "        except:\n",
    "            return None\n",
    "\n",
    "    dct = {\n",
    "        'ACCURACY': metrics.accuracy_score(y_true, y_pred),\n",
    "        'F1': metrics.f1_score(y_true, y_pred, average='weighted', pos_label=pos_label),\n",
    "        'F1_MICRO': metrics.f1_score(y_true, y_pred, average='micro', pos_label=pos_label),\n",
    "        'F1_MACRO': metrics.f1_score(y_true, y_pred, average='macro', pos_label=pos_label),\n",
    "        'PRECISION': metrics.precision_score(y_true, y_pred, average='binary', pos_label=pos_label),\n",
    "        'RECALL': metrics.recall_score(y_true, y_pred, average='binary', pos_label=pos_label),\n",
    "        'AUC': metric_auc(y_true, y_pred),\n",
    "        'KAPPA': metrics.cohen_kappa_score(y_true, y_pred)\n",
    "    }\n",
    "    return {k: 0.0 if not v else v for k, v in dct.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para usar como entrada em uma MLP simples, temos de converter de lista de tensores para lista de listas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = [torch.from_numpy(np.array(x[0])) for x in testset]\n",
    "y_test = [x[1] for x in testset]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = [1 if ri_model(x) > 0.5 else 0 for x in X_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.7/site-packages/sklearn/metrics/_ranking.py:813: UndefinedMetricWarning: No positive samples in y_true, true positive value should be meaningless\n",
      "  UndefinedMetricWarning)\n",
      "/usr/local/lib/python3.7/site-packages/sklearn/metrics/_classification.py:620: RuntimeWarning: invalid value encountered in true_divide\n",
      "  k = np.sum(w_mat * confusion) / np.sum(w_mat * expected)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'ACCURACY': 1.0,\n",
       " 'F1': 1.0,\n",
       " 'F1_MICRO': 1.0,\n",
       " 'F1_MACRO': 1.0,\n",
       " 'PRECISION': 0.0,\n",
       " 'RECALL': 0.0,\n",
       " 'AUC': nan,\n",
       " 'KAPPA': nan}"
      ]
     },
     "execution_count": 347,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_binary_classification(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
