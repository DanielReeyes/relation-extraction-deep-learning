{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O que é <b>BERT</b>?\n",
    "\n",
    "<b>BERT</b> significa <b>B</b>idirectional <b>E</b>ncoder <b>R</b>epresentations from <b>T</b>ransformers.\n",
    "\n",
    "<b>Bidirecional</b> - para entender o texto que estamos procurando, temos de olhar para trás (nas palavras anteriores) e para a frente (nas próximas palavras)\n",
    "\n",
    "<b>Transformers</b> - O mecanismo de Atenção é tudo que precisamos. O Transformer lê sequências inteiras de tokens de uma vez. Em certo sentido, o modelo é não direcional, enquanto os LSTMs leem sequencialmente (da esquerda para a direita ou da direita para a esquerda). O mecanismo de atenção permite o aprendizado de relações contextuais entre palavras.\n",
    "\n",
    "<b>Embeddings</b> de palavras contextualizadas (pré-treinadas) - O artigo ELMO apresentou uma maneira de codificar palavras com base em seu significado / contexto.\n",
    "\n",
    "O BERT foi treinado mascarando 15% dos tokens com o objetivo de adivinhá-los. Um objetivo adicional era prever a próxima frase.\n",
    "\n",
    "BERT é simplesmente uma stack pré-treinada de Transformer Encoders com duas versões \n",
    "- com 12 (BERT base) \n",
    "- com 24 (BERT Large)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A BERT pode ser utilizada em inúmeras tasks do NLP, aqui <b>tentaremos</b> utilizar para classificar se, dadas duas entidades e a sentença a qual elas pertencem, há uma relação semântica entre elas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers==3.3.1\n",
      "  Downloading transformers-3.3.1-py3-none-any.whl (1.1 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.1 MB 12.5 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: numpy in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from transformers==3.3.1) (1.18.1)\n",
      "Collecting tokenizers==0.8.1.rc2\n",
      "  Downloading tokenizers-0.8.1rc2-cp36-cp36m-manylinux1_x86_64.whl (3.0 MB)\n",
      "\u001b[K     |████████████████████████████████| 3.0 MB 64.5 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: filelock in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from transformers==3.3.1) (3.0.12)\n",
      "Collecting sentencepiece!=0.1.92\n",
      "  Downloading sentencepiece-0.1.91-cp36-cp36m-manylinux1_x86_64.whl (1.1 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.1 MB 58.6 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting dataclasses; python_version < \"3.7\"\n",
      "  Downloading dataclasses-0.7-py3-none-any.whl (18 kB)\n",
      "Collecting regex!=2019.12.17\n",
      "  Downloading regex-2020.9.27-cp36-cp36m-manylinux2010_x86_64.whl (662 kB)\n",
      "\u001b[K     |████████████████████████████████| 662 kB 58.5 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting sacremoses\n",
      "  Downloading sacremoses-0.0.43.tar.gz (883 kB)\n",
      "\u001b[K     |████████████████████████████████| 883 kB 66.6 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from transformers==3.3.1) (4.42.1)\n",
      "Requirement already satisfied: requests in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from transformers==3.3.1) (2.22.0)\n",
      "Requirement already satisfied: packaging in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from transformers==3.3.1) (20.1)\n",
      "Requirement already satisfied: six in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from sacremoses->transformers==3.3.1) (1.14.0)\n",
      "Requirement already satisfied: click in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from sacremoses->transformers==3.3.1) (7.0)\n",
      "Requirement already satisfied: joblib in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from sacremoses->transformers==3.3.1) (0.14.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from requests->transformers==3.3.1) (2020.6.20)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from requests->transformers==3.3.1) (3.0.4)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from requests->transformers==3.3.1) (1.25.8)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from requests->transformers==3.3.1) (2.8)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from packaging->transformers==3.3.1) (2.4.6)\n",
      "Building wheels for collected packages: sacremoses\n",
      "  Building wheel for sacremoses (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for sacremoses: filename=sacremoses-0.0.43-py3-none-any.whl size=893259 sha256=83f6e6944e83303ed43cce70eaef31771c303a85b1afc7ff9b97e713e87367dd\n",
      "  Stored in directory: /home/ec2-user/.cache/pip/wheels/49/25/98/cdea9c79b2d9a22ccc59540b1784b67f06b633378e97f58da2\n",
      "Successfully built sacremoses\n",
      "Installing collected packages: tokenizers, sentencepiece, dataclasses, regex, sacremoses, transformers\n",
      "Successfully installed dataclasses-0.7 regex-2020.9.27 sacremoses-0.0.43 sentencepiece-0.1.91 tokenizers-0.8.1rc2 transformers-3.3.1\n",
      "\u001b[33mWARNING: You are using pip version 20.0.2; however, version 20.2.3 is available.\n",
      "You should consider upgrading via the '/home/ec2-user/anaconda3/envs/python3/bin/python -m pip install --upgrade pip' command.\u001b[0m\n",
      "Collecting torch==1.6.0\n",
      "  Downloading torch-1.6.0-cp36-cp36m-manylinux1_x86_64.whl (748.8 MB)\n",
      "\u001b[K     |████████████████████████████████| 748.8 MB 5.4 kB/s  eta 0:00:01    |████                            | 93.3 MB 66.9 MB/s eta 0:00:10     |█████████████▏                  | 306.8 MB 79.2 MB/s eta 0:00:06\n",
      "\u001b[?25hRequirement already satisfied: future in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from torch==1.6.0) (0.18.2)\n",
      "Requirement already satisfied: numpy in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from torch==1.6.0) (1.18.1)\n",
      "Installing collected packages: torch\n",
      "Successfully installed torch-1.6.0\n",
      "\u001b[33mWARNING: You are using pip version 20.0.2; however, version 20.2.3 is available.\n",
      "You should consider upgrading via the '/home/ec2-user/anaconda3/envs/python3/bin/python -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "#Trecho para ser executado quando no Sagemaker\n",
    "!pip install transformers==3.3.1\n",
    "!pip install torch==1.6.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importando as bibliotecas necessárias para rodar o modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "from transformers import BertModel, BertTokenizer, AdamW, get_linear_schedule_with_warmup\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from pylab import rcParams\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import rc\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from collections import defaultdict\n",
    "from textwrap import wrap\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setando alguns parametros básicos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "#Configurando parametros para visualização de dados\n",
    "sns.set(style='whitegrid', palette='muted', font_scale=1.2)\n",
    "HAPPY_COLORS_PALETTE = [\"#01BEFE\", \"#FFDD00\", \"#FF7D00\", \"#FF006D\", \"#ADFF02\", \"#8F00FF\"]\n",
    "sns.set_palette(sns.color_palette(HAPPY_COLORS_PALETTE))\n",
    "rcParams['figure.figsize'] = 12, 8\n",
    "class_names = ['negativa', 'positiva']\n",
    "\n",
    "#Configuração de SEED \n",
    "RANDOM_SEED = 42\n",
    "np.random.seed(RANDOM_SEED)\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "\n",
    "#Flag para utilizar cuda quando presente uma GPU ou utilizar CPU\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "device = \"cpu\"\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Capturando os dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/SageMaker/relation-extraction-deep-learning//data\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentenca</th>\n",
       "      <th>entidade1</th>\n",
       "      <th>entidade1_tipo</th>\n",
       "      <th>relacao</th>\n",
       "      <th>entidade2</th>\n",
       "      <th>entidade2_tipo</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A Kroton anunciou uma parceria com o Cubo Itaú...</td>\n",
       "      <td>Kroton</td>\n",
       "      <td>ORG</td>\n",
       "      <td>anunciou uma parceria com</td>\n",
       "      <td>Cubo Itaú</td>\n",
       "      <td>ORG</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>O evento Summit AgriHub reuniu nesta quinta-fe...</td>\n",
       "      <td>Summit AgriHub</td>\n",
       "      <td>ORG</td>\n",
       "      <td>reuniu em</td>\n",
       "      <td>Cuiabá</td>\n",
       "      <td>PLC</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>O evento Summit AgriHub reuniu nesta quinta-fe...</td>\n",
       "      <td>Summit AgriHub</td>\n",
       "      <td>ORG</td>\n",
       "      <td>None</td>\n",
       "      <td>Mato Grosso</td>\n",
       "      <td>PLC</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>O evento Summit AgriHub reuniu nesta quinta-fe...</td>\n",
       "      <td>Summit AgriHub</td>\n",
       "      <td>ORG</td>\n",
       "      <td>None</td>\n",
       "      <td>Brasil</td>\n",
       "      <td>PLC</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Ousuário consegue comparar prazos , condições ...</td>\n",
       "      <td>Bradesco</td>\n",
       "      <td>ORG</td>\n",
       "      <td>None</td>\n",
       "      <td>Banco do Brasil</td>\n",
       "      <td>ORG</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            sentenca       entidade1  \\\n",
       "0  A Kroton anunciou uma parceria com o Cubo Itaú...          Kroton   \n",
       "1  O evento Summit AgriHub reuniu nesta quinta-fe...  Summit AgriHub   \n",
       "2  O evento Summit AgriHub reuniu nesta quinta-fe...  Summit AgriHub   \n",
       "3  O evento Summit AgriHub reuniu nesta quinta-fe...  Summit AgriHub   \n",
       "4  Ousuário consegue comparar prazos , condições ...        Bradesco   \n",
       "\n",
       "  entidade1_tipo                    relacao        entidade2 entidade2_tipo  \\\n",
       "0            ORG  anunciou uma parceria com        Cubo Itaú            ORG   \n",
       "1            ORG                  reuniu em           Cuiabá            PLC   \n",
       "2            ORG                       None      Mato Grosso            PLC   \n",
       "3            ORG                       None           Brasil            PLC   \n",
       "4            ORG                       None  Banco do Brasil            ORG   \n",
       "\n",
       "   class  \n",
       "0      1  \n",
       "1      1  \n",
       "2      0  \n",
       "3      0  \n",
       "4      0  "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Trecho para ser executado via Sagemaker\n",
    "pwd = os.path.join(os.getcwd(), 'data').replace(\"notebook\",\"\")\n",
    "\n",
    "#pwd = os.path.join(os.getcwd(), 'data').replace(\"notebook\\\\\",\"\")\n",
    "print(pwd)\n",
    "\n",
    "data = pd.read_csv(os.path.join(pwd, 'input/data_full_tmp.csv'), sep=';').replace({np.NaN: None})\n",
    "data.columns = ['sentenca','entidade1','entidade1_tipo','relacao','entidade2','entidade2_tipo']\n",
    "\n",
    "data['class'] = data['relacao'].apply(lambda x: 0 if x is None else 1)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verificando como o dataset está populado.\n",
    "Não informações nulas nos campos de Sentença, Entidade 1, Entidade 2 e Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2142 entries, 0 to 2141\n",
      "Data columns (total 7 columns):\n",
      " #   Column          Non-Null Count  Dtype \n",
      "---  ------          --------------  ----- \n",
      " 0   sentenca        2142 non-null   object\n",
      " 1   entidade1       2142 non-null   object\n",
      " 2   entidade1_tipo  2142 non-null   object\n",
      " 3   relacao         1074 non-null   object\n",
      " 4   entidade2       2142 non-null   object\n",
      " 5   entidade2_tipo  2142 non-null   object\n",
      " 6   class           2142 non-null   int64 \n",
      "dtypes: int64(1), object(6)\n",
      "memory usage: 117.3+ KB\n"
     ]
    }
   ],
   "source": [
    "#data = data.head(100)\n",
    "data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verificando o balanceamento de classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x576 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ax = sns.countplot(data['class'])\n",
    "plt.xlabel('Class Rate')\n",
    "ax.set_xticklabels(class_names);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Pre-Processamento</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "É preciso converter texto em números. \n",
    "O BERT exige ainda mais atenção. Dessa maneira:\n",
    "\n",
    "- Adicionaremos tokens especiais para separar frases e fazer a classificação\n",
    "- Passaremos sequências de comprimento constante (introduzir preenchimento)\n",
    "- Criaremos uma matriz de 0s (token de teclado) e 1s (token real) chamada máscara de atenção\n",
    "- A biblioteca Transformers fornece vários modelos de Transformers (incluindo BERT). Funciona com TensorFlow e PyTorch! Também inclui tokenizers pré-construídos que fazem o trabalho pesado para nós!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Carregando o tokenizador pre-treinado da BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "PRE_TRAINED_MODEL_NAME = 'neuralmind/bert-large-portuguese-cased'\n",
    "tokenizer = BertTokenizer.from_pretrained(PRE_TRAINED_MODEL_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Entendendo o que faz o tokenizador, pegando apenas uma sentença do corpus inteiro."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Sentença: A Kroton anunciou uma parceria com o Cubo Itaú , que dará origem à vertical “Cubo Education” , um espaço dedicado ao fomento de edtechs – tecnologia aplicada à educação .\n",
      "- Tokens: ['A', 'K', '##ro', '##ton', 'anunciou', 'uma', 'parceria', 'com', 'o', 'Cub', '##o', 'Ita', '##ú', ',', 'que', 'dar', '##á', 'origem', 'à', 'vertical', '“', 'Cub', '##o', 'Educa', '##tion', '”', ',', 'um', 'espaço', 'dedicado', 'ao', 'fome', '##nto', 'de', 'ed', '##tec', '##h', '##s', '–', 'tecnologia', 'aplicada', 'à', 'educação', '.']\n",
      "- Token IDs: [177, 461, 157, 897, 3127, 230, 4495, 170, 146, 18842, 22280, 4008, 22332, 117, 179, 2822, 22303, 2008, 353, 14357, 1112, 18842, 22280, 14191, 1131, 22354, 117, 222, 2363, 8055, 320, 11062, 234, 125, 902, 8948, 22296, 22281, 1379, 4277, 11107, 353, 3478, 119]\n"
     ]
    }
   ],
   "source": [
    "sentenca_amostra = data['sentenca'].iloc[0]\n",
    "tokens = tokenizer.tokenize(sentenca_amostra)\n",
    "token_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "print(f'- Sentença: {sentenca_amostra}')\n",
    "print(f'- Tokens: {tokens}')\n",
    "print(f'- Token IDs: {token_ids}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Tokens Especiais </h2>\n",
    "\n",
    "[SEP] - Token que marca o fim da sentença.\n",
    "('[SEP]', 102)\n",
    "\n",
    "[CLS] - Token que marca o início da sentença.\n",
    "('[CLS]', 101)\n",
    "\n",
    "[PAD] - Token para preenchimento da sentença para que todas fiquem em um cumprimento fixo.\n",
    "('[PAD]', 0)\n",
    "\n",
    "[UNK] - Token para todos os tokens que a BERT desconhece\n",
    "('[UNK]', 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Estabelecendo o comprimento da sentença</h2>\n",
    "\n",
    "O BERT trabalha com sequências de comprimento fixo. \n",
    "Vamos armazenar o comprimento do token de cada avaliação e capturar o comprimento máximo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "156\n"
     ]
    }
   ],
   "source": [
    "token_lens = []\n",
    "for txt in data.sentenca:\n",
    "    tokens = tokenizer.encode(txt, max_length=512)\n",
    "    token_lens.append(len(tokens))\n",
    "    if len(tokens) == 192:\n",
    "        print(txt)\n",
    "    \n",
    "MAX_LENGHT = max(token_lens)\n",
    "print(MAX_LENGHT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Histograma dos tokens\n",
    "\n",
    "para visualizar a distribuição das sentenças em relação a distribuição dos tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x576 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.distplot(token_lens)\n",
    "plt.xlim([0, 256]);\n",
    "plt.xlabel('Token count');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Classe de pré-processamento</h2>\n",
    "    \n",
    "Temos todos os blocos de construção necessários para criar um conjunto de dados PyTorch. Isso será feito a partir de uma classe estabelecida abaixo.\n",
    "\n",
    "O tokenizer está fazendo a maior parte do trabalho. Também devolve a sentença que foi classificada, para que seja mais fácil avaliar as previsões do modelo. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO Fazer a passagem das entidades como parametro para concatenar os vetores delas ao vetor da sentença\n",
    "class preProccesDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, sentence, entity1, entity2, targets, tokenizer, max_len):\n",
    "        self.sentence = sentence\n",
    "        self.entity1 = entity1\n",
    "        self.entity2 = entity2\n",
    "        self.targets = targets\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.sentence)\n",
    "    \n",
    "    def __getitem__(self, item):\n",
    "        inputs = [str(self.sentence[item]), str(self.entity1[item]), str(self.entity2[item])]\n",
    "        target = self.targets[item]\n",
    "        # Encodamos todas as entradas\n",
    "        encoding = [\n",
    "        self.tokenizer.encode_plus(\n",
    "        x,\n",
    "        add_special_tokens=True,\n",
    "        max_length=self.max_len,\n",
    "        return_token_type_ids=True,\n",
    "        pad_to_max_length=True,\n",
    "        return_attention_mask=True,\n",
    "        return_tensors='pt',\n",
    "        truncation=True,\n",
    "        )\n",
    "        for x in inputs\n",
    "        ]\n",
    "\n",
    "        # Contatenamos os ids e attention_masks das sentencas e entidades\\n\",\n",
    "        input_ids = torch.cat(tuple(x['input_ids'].flatten() for x in encoding), dim=0)\n",
    "        attention_mask = torch.cat(tuple(x['attention_mask'].flatten() for x in encoding), dim=0)\n",
    "        \n",
    "        return {\n",
    "          'input_text': inputs,\n",
    "          'input_ids': input_ids,\n",
    "          'attention_mask': attention_mask,\n",
    "          'targets': torch.tensor(target, dtype=torch.long)\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Separação dos dados</h2>\n",
    "Treino, Teste e Validação\n",
    "\n",
    "Dividindo os dados entre treino (x%) e teste (y%), onde x+y = 100\n",
    "\n",
    "Em seguida, divide os dados de teste entre teste (x%) e validação (y%), onde x+y = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1713, 7), (215, 7), (214, 7))"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train, df_test = train_test_split(\n",
    "  data,\n",
    "  test_size=0.2,\n",
    "  random_state=RANDOM_SEED\n",
    ")\n",
    "\n",
    "df_val, df_test = train_test_split(\n",
    "  df_test,\n",
    "  test_size=0.5,\n",
    "  random_state=RANDOM_SEED\n",
    ")\n",
    "\n",
    "df_train.shape, df_test.shape, df_val.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Data Loader </h2>\n",
    "Função que auxilia na criação dos dataloaders dos dados utilizando a classe recém criada de processamento de sentenças."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_data_loader(df, tokenizer, max_len, batch_size):\n",
    "    ds = preProccesDataset(\n",
    "        sentence=df['sentenca'].to_numpy(),\n",
    "        entity1=df['entidade1'].to_numpy(),\n",
    "        entity2=df['entidade2'].to_numpy(),\n",
    "        targets=df['class'].to_numpy(),\n",
    "        tokenizer=tokenizer,\n",
    "        max_len=max_len\n",
    "  )\n",
    "    return DataLoader(\n",
    "        ds,\n",
    "        batch_size=batch_size,\n",
    "        num_workers=0\n",
    "  )\n",
    "\n",
    "BATCH_SIZE = 16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Criando os data loaders para cada função de dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_loader = create_data_loader(df_train, tokenizer, MAX_LENGHT, BATCH_SIZE)\n",
    "val_data_loader = create_data_loader(df_val, tokenizer, MAX_LENGHT, BATCH_SIZE)\n",
    "test_data_loader = create_data_loader(df_test, tokenizer, MAX_LENGHT, BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exemplo do data loader de treinamento:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['input_text', 'input_ids', 'attention_mask', 'targets'])\n",
      "torch.Size([16, 468])\n",
      "torch.Size([16, 468])\n",
      "torch.Size([16])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/tokenization_utils_base.py:1773: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n"
     ]
    }
   ],
   "source": [
    "df = next(iter(train_data_loader))\n",
    "print(df.keys())\n",
    "\n",
    "print(df['input_ids'].shape)\n",
    "print(df['attention_mask'].shape)\n",
    "print(df['targets'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_text': [('Cleo irá realizar serviços como consultas de saldos e limites , descrição de compras realizadas no cartão , emissão de 2ª via de boleto , parcelamento de fatura , entre outros A administradora de cartões de crédito para o mercado varejista , Cred-System , firmou parceria com a Neobpo , empresa especializada na oferta de soluções , tecnologia e serviços de BPO , para desenvolver um Assistente Virtual Inteligente',\n",
       "   \"Em 06/11/1982 foi oficialmente inaugurado o prédio principal, tendo como patrono o Dr. José Getúlio Lima (advogado e professor, pai do Dr. José Carlos de Barros Lima), que atuou muito na área educacional da região tendo sido o fundador, entre outros, do ginásio 'Conselheiro Lafayette' (1942) posteriormente 'Ginásio Estadual Anhanguera' e atualmente E.E.S.G. 'Pereira Barreto'.\",\n",
       "   \"Para analistas como Kris Voorspools, da Fortis, em Bruxelas, o 'preço do petróleo pode continuar a subir', por se estar perante o 'simples fundamento da oferta e da procura'.\",\n",
       "   'A empresa faturou 20 milhões de reais em 2018 e recebeu neste ano aporte de 5 milhões de reais do fundo Kviv Ventures , de Raphael Klein , da família fundadora da Casas Bahia .',\n",
       "   'Agora , com mais de um milhão de clientes e uma avaliação superior a £ 1 bilhão , a Monzo Em 2019 , a Finestra acredita que a Monzo começa a dar passos no sentido de se tornar lucrativa pela primeira vez .',\n",
       "   'Em Portugal , o crédito com desconto em folha já é oferecido em parceria com o banco BNI , que inovou ao adotar o modelo em parceria com a SalaryFits .',\n",
       "   'O Intesa Sanpaolo é o último a fazer testes com impressões digitais , em colaboração com a Gemalto e com a startup nórdica Zwipe .',\n",
       "   'Com 400 drones , a frota da Corteva Agriscience , divisão agrícola da DowDuPont , é a maior focada em agricultura do mundo – e agora usará a tecnologia da DroneDeploy , responsável por um sistema de mapeamento de alta precisão .',\n",
       "   'A Odontoclinic , rede nacional de franquias de clínicas odontológicas , acaba de fechar uma parceria com o Santander Brasil que permitirá o acesso a tratamentos odontológicos de qualidade com o pagamento facilitado .',\n",
       "   'Sunab autua empresas por alta abusiva dos preços Da Sucursal de Brasília e da Reportagem Local A Sunab (Superintendência Nacional de Abastecimento) autuou 62 estabelecimentos comerciais em 16 Estados entre 27 de junho e 8 de julho último.',\n",
       "   \"Organizada pelo Núcleo Interactivo de Astronomia (NUCLIO) no Centro de Interpretação Ambiental da Ponta do Sal, em São Pedro do Estoril, a quarta edição da iniciativa inclui sessões de observação do sol e do céu nocturno, oficinas de astronomia, lançamentos de foguetões de água, palestras e uma 'procura de asteróides', actividade que a instituição promove regularmente em as escolas do concelho.\",\n",
       "   \"Por o mesmo diapasão alinhou o delegado em Lagoa da Associação de Agricultores de Famalicão, António Ribeiro, em declarações ao Público: 'A Câmara nunca falou com nós, fizeram tudo em as costas das pessoas'.\",\n",
       "   'A iZettle está em 26º lugar entre as 1000 fintechs mais inovadoras do mundo , segundo ranking da consultoria KPMG , e em 21º lugar entre as 1. 0 empresas que mais crescem na Europa .',\n",
       "   '\"Aprendemos que , quando criamos para a Índia , criamos para o mundo\" , disse Caesar Sengupta , vice-presidente da iniciativa do Google Next Billion Users e do Payments , no evento \"Google for Índia \" em Nova Déli , na terça-feira .',\n",
       "   \"Em o próximo sábado, a as 16h, Ronaldo Lemos, diretor do Creative Commons no Brasil, irá participar de um debate com os membros do Superflex, na Vermelho, e logo após será realizada uma gincana intitulada 'Free Beer Pub Quiz', na qual participantes receberão prêmios ao responderem questões sobre propriedade intelectual e direitos autorais.\",\n",
       "   'Rodrigo Ventura decidiu criar a 88i após validar sua tese no Startup Weekend Blockchain Techstars , evento patrocinado pelo Google e pela TechStars , braço de inovação do banco Barclays da Inglaterra .'),\n",
       "  ('Cred-System',\n",
       "   'Dr. José Getúlio Lima',\n",
       "   'Fortis',\n",
       "   'Kviv Ventures',\n",
       "   'Monzo',\n",
       "   'SalaryFits',\n",
       "   'Gemalto',\n",
       "   'Corteva Agriscience',\n",
       "   'Odontoclinic',\n",
       "   'Da Sucursal de Brasília',\n",
       "   'Núcleo Interactivo de Astronomia',\n",
       "   'Lagoa',\n",
       "   'iZettle',\n",
       "   'Google Next Billion Users',\n",
       "   'Creative Commons',\n",
       "   'TechStars'),\n",
       "  (' BPO',\n",
       "   'Conselheiro Lafayette',\n",
       "   'Bruxelas',\n",
       "   'Casas Bahia',\n",
       "   'Finestra',\n",
       "   ' Portugal',\n",
       "   'Intesa Sanpaolo',\n",
       "   'DowDuPont',\n",
       "   ' Santander Brasil',\n",
       "   'Reportagem Local',\n",
       "   'Centro de Interpretação Ambiental da Ponta do Sal',\n",
       "   'Associação de Agricultores de Famalicão',\n",
       "   ' Europa',\n",
       "   ' Caesar Sengupta',\n",
       "   'Brasil',\n",
       "   'Rodrigo Ventura')],\n",
       " 'input_ids': tensor([[  101,  5624, 22280,  ...,     0,     0,     0],\n",
       "         [  101,   335, 18838,  ...,     0,     0,     0],\n",
       "         [  101,   959,  6502,  ...,     0,     0,     0],\n",
       "         ...,\n",
       "         [  101,   107, 19753,  ...,     0,     0,     0],\n",
       "         [  101,   335,   146,  ...,     0,     0,     0],\n",
       "         [  101,  7697, 18072,  ...,     0,     0,     0]]),\n",
       " 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         ...,\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0]]),\n",
       " 'targets': tensor([0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0])}"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = next(iter(train_data_loader))\n",
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Carregando o modelo pré-treinado BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_model = BertModel.from_pretrained(PRE_TRAINED_MODEL_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Modelo Classificador</h2>\n",
    "\n",
    "Criando um classificador que use o modelo <b>BERT</b>:\n",
    "\n",
    "O classificador delega a maior parte do trabalho pesado para o BertModel. Usamos uma camada de eliminação para alguma regularização e uma camada totalmente conectada para nossa saída. \n",
    "\n",
    "Retorna a saída bruta da última camada, pois isso é necessário para que a função de perda de entropia cruzada em PyTorch funcione."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModeloVerificaRelacao(nn.Module):\n",
    "    \n",
    "    def __init__(self, n_classes):\n",
    "        super(ModeloVerificaRelacao, self).__init__()\n",
    "        self.bert = BertModel.from_pretrained(PRE_TRAINED_MODEL_NAME)\n",
    "        self.drop = nn.Dropout(p=0.3)\n",
    "        self.out = nn.Linear(self.bert.config.hidden_size, n_classes)\n",
    "        \n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        _, pooled_output = self.bert(\n",
    "          input_ids=input_ids,\n",
    "          attention_mask=attention_mask\n",
    "    )\n",
    "    \n",
    "        output = self.drop(pooled_output)\n",
    "        return self.out(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Criando uma instância e movê-la para a GPU <b>(caso tenha)</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "NUM_CLASSES = len(class_names)\n",
    "print(NUM_CLASSES)\n",
    "\n",
    "model = ModeloVerificaRelacao(NUM_CLASSES)\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lote de exemplo de nossos dados de treinamento movido para a GPU:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 468])\n",
      "torch.Size([16, 468])\n"
     ]
    }
   ],
   "source": [
    "input_ids = df['input_ids'].to(device)\n",
    "attention_mask = df['attention_mask'].to(device)\n",
    "\n",
    "print(input_ids.shape) # batch size x seq length\n",
    "print(attention_mask.shape) # batch size x seq length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Treinamento</h2>\n",
    "\n",
    "Para reproduzir o procedimento de treinamento do artigo de BERT, usamos o otimizador AdamW fornecido por Hugging Face. Ele corrige a deterioração do peso. Também usaremos um agendador linear sem etapas de warmup:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 10\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=2e-5, correct_bias=False)\n",
    "total_steps = len(train_data_loader) * EPOCHS\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "  optimizer,\n",
    "  num_warmup_steps=0,\n",
    "  num_training_steps=total_steps\n",
    ")\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss().to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "De onde são esses hiperparâmetros? Os autores do BERT têm algumas recomendações para o ajuste fino:\n",
    "\n",
    "- Tamanho do lote: 16, 32\n",
    "- Taxa de aprendizagem (Adam): 5e-5, 3e-5, 2e-5\n",
    "- Número de épocas: 2, 3, 4\n",
    "- Vamos ignorar a recomendação do número de épocas, mas ficar com o resto. Observe que aumentar o tamanho do lote reduz significativamente o tempo de treinamento, mas oferece menor precisão.\n",
    "\n",
    "Função auxiliar para treinar o modelo para uma época:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, data_loader, loss_fn, optimizer, device, scheduler, n_examples):\n",
    "    model = model.train()\n",
    "    losses = []\n",
    "    correct_predictions = 0\n",
    "    for d in data_loader:\n",
    "        input_ids = d[\"input_ids\"].to(device)\n",
    "        attention_mask = d[\"attention_mask\"].to(device)\n",
    "        targets = d[\"targets\"].to(device)\n",
    "        outputs = model(\n",
    "        input_ids=input_ids,\n",
    "        attention_mask=attention_mask\n",
    "        )\n",
    "    \n",
    "    _, preds = torch.max(outputs, dim=1)\n",
    "    loss = loss_fn(outputs, targets)\n",
    "    correct_predictions += torch.sum(preds == targets)\n",
    "    losses.append(loss.item())\n",
    "    loss.backward()\n",
    "    nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "    optimizer.step()\n",
    "    scheduler.step()\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    return correct_predictions.double() / n_examples, np.mean(losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O treinamento do modelo deve parecer familiar, exceto por duas coisas. O scheduler é chamado sempre que um lote é alimentado para o modelo. A explosão de gradientes está sendo evitada recortando os gradientes do modelo usando <i><b>clipgrad_norm</b></i>.\n",
    "\n",
    "Outra função para avaliar o modelo em um determinado data loader:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_model(model, data_loader, loss_fn, device, n_examples):\n",
    "\n",
    "    model = model.eval()\n",
    "    losses = []\n",
    "    correct_predictions = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for d in data_loader:\n",
    "            input_ids = d[\"input_ids\"].to(device)\n",
    "            attention_mask = d[\"attention_mask\"].to(device)\n",
    "            targets = d[\"targets\"].to(device)\n",
    "            outputs = model(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask\n",
    "            )\n",
    "        _, preds = torch.max(outputs, dim=1)\n",
    "        loss = loss_fn(outputs, targets)\n",
    "        correct_predictions += torch.sum(preds == targets)\n",
    "        losses.append(loss.item())\n",
    "\n",
    "    return correct_predictions.double() / n_examples, np.mean(losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usando essas duas funções, temos o loop de treinamento. \n",
    "Também armazenaremos o histórico de treinamento:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "----------\n",
      "Train loss 0.740505576133728 accuracy 0.0\n",
      "Val   loss 0.751995861530304 accuracy 0.009345794392523364\n",
      "\n",
      "Epoch 2/10\n",
      "----------\n",
      "Train loss 0.2752358615398407 accuracy 0.0005837711617046118\n",
      "Val   loss 1.0187206268310547 accuracy 0.009345794392523364\n",
      "\n",
      "Epoch 3/10\n",
      "----------\n",
      "Train loss 0.15778489410877228 accuracy 0.0005837711617046118\n",
      "Val   loss 1.4615579843521118 accuracy 0.009345794392523364\n",
      "\n",
      "Epoch 4/10\n",
      "----------\n",
      "Train loss 0.11823137104511261 accuracy 0.0005837711617046118\n",
      "Val   loss 1.9618453979492188 accuracy 0.009345794392523364\n",
      "\n",
      "Epoch 5/10\n",
      "----------\n",
      "Train loss 0.04650561884045601 accuracy 0.0005837711617046118\n",
      "Val   loss 2.4828264713287354 accuracy 0.009345794392523364\n",
      "\n",
      "Epoch 6/10\n",
      "----------\n",
      "Train loss 0.008610148914158344 accuracy 0.0005837711617046118\n",
      "Val   loss 2.9644248485565186 accuracy 0.009345794392523364\n",
      "\n",
      "Epoch 7/10\n",
      "----------\n",
      "Train loss 0.0035223839804530144 accuracy 0.0005837711617046118\n",
      "Val   loss 3.452624559402466 accuracy 0.009345794392523364\n",
      "\n",
      "Epoch 8/10\n",
      "----------\n",
      "Train loss 0.0012821081327274442 accuracy 0.0005837711617046118\n",
      "Val   loss 3.870981216430664 accuracy 0.009345794392523364\n",
      "\n",
      "Epoch 9/10\n",
      "----------\n",
      "Train loss 0.0010555178159847856 accuracy 0.0005837711617046118\n",
      "Val   loss 4.272642135620117 accuracy 0.009345794392523364\n",
      "\n",
      "Epoch 10/10\n",
      "----------\n",
      "Train loss 0.0006556744920089841 accuracy 0.0005837711617046118\n",
      "Val   loss 4.6545281410217285 accuracy 0.009345794392523364\n",
      "\n",
      "CPU times: user 1d 18h 26min, sys: 25min 44s, total: 1d 18h 51min 45s\n",
      "Wall time: 2h 41min 17s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "history = defaultdict(list)\n",
    "best_accuracy = 0\n",
    "for epoch in range(EPOCHS):\n",
    "    print(f'Epoch {epoch + 1}/{EPOCHS}')\n",
    "    print('-' * 10)\n",
    "    train_acc, train_loss = train_epoch(\n",
    "        model,\n",
    "        train_data_loader,\n",
    "        loss_fn,\n",
    "        optimizer,\n",
    "        device,\n",
    "        scheduler,\n",
    "        len(df_train)\n",
    "    )\n",
    "  \n",
    "    print(f'Train loss {train_loss} accuracy {train_acc}')\n",
    "    val_acc, val_loss = eval_model(\n",
    "        model,\n",
    "        val_data_loader,\n",
    "        loss_fn,\n",
    "        device,\n",
    "        len(df_val)\n",
    "    )\n",
    "    \n",
    "    print(f'Val   loss {val_loss} accuracy {val_acc}')\n",
    "    print()\n",
    "  \n",
    "    history['train_acc'].append(train_acc)\n",
    "    history['train_loss'].append(train_loss)\n",
    "    history['val_acc'].append(val_acc)\n",
    "    history['val_loss'].append(val_loss)\n",
    "  \n",
    "    if val_acc > best_accuracy:\n",
    "        torch.save(model.state_dict(), 'best_model_state.bin')\n",
    "        best_accuracy = val_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O estado do melhor modelo é armazenado, indicado pela maior precisão de validação.\n",
    "\n",
    "É possível observar a precisão do treinamento vs validação:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x576 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(history['train_acc'], label='train accuracy')\n",
    "plt.plot(history['val_acc'], label='validation accuracy')\n",
    "plt.title('Training history')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend()\n",
    "plt.ylim([0, 1]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Avaliação do Modelo</h2>\n",
    "\n",
    "Avaliação do modelo treinado. Cálculo da acurácia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/tokenization_utils_base.py:1773: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_acc, _ = eval_model(\n",
    "  model,\n",
    "  test_data_loader,\n",
    "  loss_fn,\n",
    "  device,\n",
    "  len(df_test)\n",
    ")\n",
    "test_acc.item()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Função que vai calcular essa acurácia automaticamente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_predictions(model, data_loader):\n",
    "    model = model.eval()\n",
    "    review_texts = []\n",
    "    predictions = []\n",
    "    prediction_probs = []\n",
    "    real_values = []\n",
    "    with torch.no_grad():\n",
    "        for d in data_loader:\n",
    "            texts = d[\"input_text\"]\n",
    "            input_ids = d[\"input_ids\"].to(device)\n",
    "            attention_mask = d[\"attention_mask\"].to(device)\n",
    "            targets = d[\"targets\"].to(device)\n",
    "            outputs = model(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask\n",
    "              )\n",
    "            _, preds = torch.max(outputs, dim=1)\n",
    "            review_texts.extend(texts)\n",
    "            predictions.extend(preds)\n",
    "            prediction_probs.extend(outputs)\n",
    "            real_values.extend(targets)\n",
    "    \n",
    "    predictions = torch.stack(predictions).cpu()\n",
    "    prediction_probs = torch.stack(prediction_probs).cpu()\n",
    "    real_values = torch.stack(real_values).cpu()\n",
    "  \n",
    "    return review_texts, predictions, prediction_probs, real_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Chamada da Função para calcular as predições</h3>\n",
    "\n",
    "Semelhante à função de avaliação, exceto que estamos armazenando o texto das sentenças e as probabilidades previstas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_review_texts, y_pred, y_pred_probs, y_test = get_predictions(\n",
    "    model,\n",
    "    test_data_loader\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Relatório da Classificação</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negativa       0.46      1.00      0.63        98\n",
      "    positiva       0.00      0.00      0.00       117\n",
      "\n",
      "    accuracy                           0.46       215\n",
      "   macro avg       0.23      0.50      0.31       215\n",
      "weighted avg       0.21      0.46      0.29       215\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, y_pred, target_names=class_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Matriz de Confusão</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x576 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def show_confusion_matrix(confusion_matrix):\n",
    "    hmap = sns.heatmap(confusion_matrix, annot=True, fmt=\"d\", cmap=\"Blues\")\n",
    "    hmap.yaxis.set_ticklabels(hmap.yaxis.get_ticklabels(), rotation=0, ha='right')\n",
    "    hmap.xaxis.set_ticklabels(hmap.xaxis.get_ticklabels(), rotation=30, ha='right')\n",
    "    plt.ylabel('Classificação Real')\n",
    "    plt.xlabel('Classificação Predita');\n",
    "    \n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "df_cm = pd.DataFrame(cm, index=class_names, columns=class_names)\n",
    "\n",
    "show_confusion_matrix(df_cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([('Segundo o Banco Central , existem mais de 20 competidores nesse mercado que até 2010 era centrado apenas em Cielo , do Banco do Brasil e Bradesco , e Rede , do Itaú Unibanco .',\n",
       "   'A. H.: Um outro pilar prende-se com a Câmara do Comércio e Industria e Industria.',\n",
       "   'Em Portugal , o crédito com desconto em folha já é oferecido em parceria com o banco BNI , que inovou ao adotar o modelo em parceria com a SalaryFits .',\n",
       "   'Realizado no Lounge Plant , o debate contou com a participação de Percival Lucena , pesquisador da IBM , Fernando Martins , conselheiro da Jacto e de startups AgTech , e Rodrigo Iafelice dos Santos , presidente da Ennexas Agribusiness .',\n",
       "   'O CONCRED será realizado entre os dias 21 e 23 de novembro , em Florianópolis (SC) .',\n",
       "   'Temendo que a Nasa tenha de aposentar seus ônibus espaciais antes mesmo de 2010, a ESA (Agência Espacial Européia) já está estudando o que fazer caso seu módulo não possa ir ao espaço.',\n",
       "   'A rede da Anova conecta , alem da Chicago Market Exchange , as bolsas BATS , Nasdaq , de Nova York e Cboe Markets .',\n",
       "   'O encontro do partido ocorreu um dia depois da OMS ter dito que Pequim tem 200 casos de Sars em vez dos 37 notificados.',\n",
       "   'O aumento da temperatura na Antártida durante o último século foi de 1,2°C, pelo menos o dobro da média no resto do planeta, que ficou entre 0,2°C e 0,6°C, segundo o IPCC (Painel Intergovernamental de Mudança Climática, órgão da ONU).',\n",
       "   \"A sua fundação, como 'Biblioteca de Marinha', deve-se à Rainha D. Maria II, por decreto de 7 de Janeiro de 1835, ficando de ínicio integrada no Arsenal Real de Marinha e instalada no andar nobre do respectivo edíficio da Ribeira das Naus.\",\n",
       "   \"Matthew, que lançou no ano passado o livro 'A conturbada história das bibliotecas' (em o Brasil, editado pela Planeta), ironiza: - Já deve ter alguém trabalhando nesta pílula. provavelmente, Bill Gates!\",\n",
       "   'A abertura do evento contou com a presença de Fernando Gomes, presidente da Câmara Municipal do Porto, e do vereador Wan Chun, em representação de Sales Marques, presidente do Leal Senado, a qual foi enriquecida com um pequeno concerto em que os instrumentistas Wong On Yuen e António Ferro, tocando ehru e viola-baixo, respectivamente, interpretaram várias peças editadas no disco compacto Sinais de Yuanju.',\n",
       "   'A Pesquisa FinTech Deep Dive 2018 , feita pela Associação Brasileira de Fintechs (A B Fintechs ) e pela consultoria PwC Brasil , divulgada esta semana , registrou otimismo das startups financeiras .',\n",
       "   'O aumento da temperatura na Antártida durante o último século foi de 1,2°C, pelo menos o dobro da média no resto do planeta, que ficou entre 0,2°C e 0,6°C, segundo o IPCC (Painel Intergovernamental de Mudança Climática, órgão da ONU).',\n",
       "   'Além de oferecer uma gama completa de serviços bancários de varejo e SME , a nova loja oferecerá : - um centro bancário de negócios - espaço de co-working e salas de reuniões privativas gratuitas para freelancers locais , empresários e pequenas empresas - uma apresentação e espaço de vitrine de negócios - o B Social Studio - permitindo às PMEs criar seus próprios conteúdos e podcasts de marketing social e digital com o apoio de especialistas no local - Um programa regular de eventos de negócios em parceria com uma série de empresas baseadas em Manchester , incluindo Boohoo.com , Manchester Metropolitan University e Freeformers .',\n",
       "   'Em janeiro deste ano , a startup anunciou um aporte de US$ 200 milhões do Central Group , o maior grupo de varejo da Tailândia .'),\n",
       "  ('Rede',\n",
       "   'A. H.',\n",
       "   'BNI',\n",
       "   'Jacto',\n",
       "   'CONCRED',\n",
       "   'Nasa',\n",
       "   'Nasdaq',\n",
       "   'OMS',\n",
       "   'ONU',\n",
       "   'Biblioteca de Marinha',\n",
       "   'Matthew',\n",
       "   'Fernando Gomes',\n",
       "   'Associação Brasileira de Fintechs (A B Fintechs',\n",
       "   'Painel Intergovernamental de Mudança Climática',\n",
       "   'SME',\n",
       "   'Central Group'),\n",
       "  (' Itaú Unibanco',\n",
       "   'Câmara do Comércio e Industria',\n",
       "   ' SalaryFits',\n",
       "   'Fernando Martins',\n",
       "   'Florianópolis',\n",
       "   'Agência Espacial Européia',\n",
       "   ' Nova York',\n",
       "   'Sars',\n",
       "   'Antártida',\n",
       "   'Arsenal Real de Marinha',\n",
       "   'Planeta',\n",
       "   'Câmara Municipal do Porto',\n",
       "   'PwC Brasil',\n",
       "   'ONU',\n",
       "   ' Manchester',\n",
       "   'Tailândia'),\n",
       "  ('De acordo com Coutinho , facilita no lançamento de novos produtos e serviços o fato de os acionistas minoritários Manzat Inversiones e Guilherme Alberto Berthier Stumpf terem exercido , no fim do ano passado , opção de venda de 11,5 % do capital social restante da Getnet para o Santander , o que estava previsto no contrato celebrado em 2014 .',\n",
       "   'A TrustHub está perto de fechar a contratação do Parcele Mais com outras três grandes empresas que possuem marketplace para negócios B2B (Buine s to business) , uma delas líder do setor farmacêutico de São Paulo e uma multinacional de produtos de limpeza .',\n",
       "   'A Arábia Saudita era um dos três países do mundo que reconheciam, no Afeganistão, o governo Talibã.',\n",
       "   \"Estamos recolhendo amostras de coprólitos [fezes fossilizadas] de duas cavernas em Israel com 40 mil anos, onde provavelmente Cro-Magnons [os primeiros humanos modernos] e neandertais viveram lado a lado', contou o pesquisador durante a reunião da AAAS (Associação Americana para o Avanço da Ciência).\",\n",
       "   'Itaú fecha parceria com OLX .',\n",
       "   'Depois de muito esforço, o doutor faz uma regressão em Maçaranduba, que resolve sua crise existencial e volta a ter a sua velha agressividade Mudanças no Megatom O diretor Aloysio Legey deixou o comando do Megatom para cuidar dos especiais de fim de ano da Globo.',\n",
       "   'A Apple deve lançar seu próprio cartão de crédito em parceria com o banco Goldman Sachs , aponta uma reportagem do The Wall Street Journal .',\n",
       "   'Almeida Henriques, presidente da Associação Industrial da Região de Viseu (AIRV), é o novo rosto do Conselho Empresarial do Centro (CEC).',\n",
       "   'Quando tomou posse , o novo presidente da Caixa , Pedro Guimarães , afirmou que o objetivo era atingir 30 milhões de clientes no microcrédito e que a nova empresa seria criada em parceria com o BNB e o Banco do Brasil .',\n",
       "   'Conforme já havia sido adiantado pela imprensa no final da semana passada , o Banco Inter publicou segunda-feira (16 ) um comunicado de fato relevante confirmando a conclusão de um novo acordo com o Softbank Group Corp .',\n",
       "   'Sem esse lado reformista, o PSD ficará tão próximo do Governo e do PS que, à luz de um, não se vê a sombra do outro.',\n",
       "   \"É o que mostra o estudo publicado pelo britânico David Vaughan e seus colegas do British Antarctic Survey na revista 'Science' de esta semana.\",\n",
       "   'A fim de evitar que seus clientes paguem pelas transferências via TED ou DOC , a fintech disponibiliza contas nos bancos Itaú , Santander , Bradesco , Banco do Brasil e Banco Inter e Caixa Econômica , para onde o investidor envia o valor desejado .',\n",
       "   'A partir de agora , a NET está na Claro .',\n",
       "   'A TrustHub , fintech do grupo SRM e que também tem uma gestora especializada em fundos de recebíveis , fechou acordo com o Grupo Ipiranga , para antecipação de recebíveis de seus clientes corporativos .',\n",
       "   '\"N o primeiro momento , o app irá destacar os pagamentos feitos a Netflix , iFood , Spotify , Uber e PayPal , os serviços digitais mais usados pelos clientes na plataforma da Neon\" , afirma a fintech em nota .'),\n",
       "  ('Getnet',\n",
       "   'B2B',\n",
       "   'Afeganistão',\n",
       "   'Associação Americana para o Avanço da Ciência',\n",
       "   'Itaú',\n",
       "   'Megatom',\n",
       "   'Apple',\n",
       "   'Almeida Henriques',\n",
       "   'Caixa',\n",
       "   'Banco Inter',\n",
       "   'PSD',\n",
       "   'British Antarctic Survey',\n",
       "   'Itaú',\n",
       "   'NET',\n",
       "   'TrustHub',\n",
       "   'iFood'),\n",
       "  (' Santander',\n",
       "   'São Paulo',\n",
       "   'Talibã',\n",
       "   'Israel',\n",
       "   'OLX',\n",
       "   'Globo',\n",
       "   ' The Wall Street Journal',\n",
       "   'Conselho Empresarial do Centro',\n",
       "   ' Banco do Brasil',\n",
       "   ' Softbank Group Corp',\n",
       "   'Governo',\n",
       "   'David Vaughan',\n",
       "   'Banco do Brasil',\n",
       "   ' Claro',\n",
       "   'SRM',\n",
       "   'Spotify'),\n",
       "  ('A Marisa , maior marca feminina de moda e lingerie do País , lança , em parceria com a Icatu Seguros , um novo produto financeiro: Marisa Previdência .',\n",
       "   'Nuno Severiano Teixeira visitará o contingente português estacionado em Campo Warehouse, composto pela 22.ªCompanhia de Atiradores Pára-Quedistas da Brigada de Reacção Rápida e uma equipa de Controladores Aéreos Avançados da Força Aérea.',\n",
       "   'O chatbot Olivia , criado por Lucas Moraes , da quarta geração de herdeiros da família Ermírio de Moraes , do grupo Votorantim , tem o Itaú e o Banco do Brasil como seus novos clientes e continua a aumentar a lista de espera para seu aplicativo de educação financeira via inteligência artificial .',\n",
       "   'Com a aquisição , a companhia busca expandir suas plataformas Einstein e Customer 360; essa foi a maior compra da história da Salesforce .',\n",
       "   'Nesse grupo , destaque para Via Varejo , Magazine Luiza , Carrefour e B2W , dona das marcas Americanas.com e Submarino.',\n",
       "   \"Francis WURTZ, da CEUE/EVN, condenou 'de maneira absoluta as declarações antieuropeias e indignas' que se ouviram no hemiciclo: 'O meu grupo está a favor de um referendo, mas opõe-se a acções como esta', afirmou.\",\n",
       "   'O Maldi feito pelo grupo do LIP (Laboratório de Instrumentação e Partículas), do Instituto de Física, também financiado pela Fapesp, saiu bem mais em conta:',\n",
       "   'Apesar de ser um dos bancos que tem sem mostrado menos amigável às plataformas de criptomoedas no Brasil , o Bradesco (que fechou a conta da exchange Braziliex ) utilizará a tecnologia da Ripple para realizar transferências internacionais para o Japão , numa parceria com o MUFG Bank , ex-Bank of Tokyo Mitsubishi UFJ .',\n",
       "   \"Para Greg Laden, da Universidade de Minnesota, e Richard Wrangham, da Universidade Harvard, raízes, tubérculos e outros USOs (sigla em inglês para 'órgãos subterrâneos de armazenamento' das plantas) permitiram que os primeiros hominídeos, como são conhecidos os ancestrais do homem, saíssem da mata fechada e colonizassem ambientes abertos a partir de uns 6 milhões de anos atrás.\",\n",
       "   'Este mesmo Governo preparou as eleições para uma Assembleia Nacional Popular que em 5 de Julho de 1975 proclamou a independência.',\n",
       "   '\"N o primeiro momento , o app irá destacar os pagamentos feitos a Netflix , iFood , Spotify , Uber e PayPal , os serviços digitais mais usados pelos clientes na plataforma da Neon\" , afirma a fintech em nota .',\n",
       "   'Recorde-se que com este concurso, a APSS também procurou envolver os estabelecimentos de ensino da região de Setúbal, dentro da sua perspectiva de abertura do porto a as escolas, já iniciada com a visita de várias turmas a as instalações portuárias.',\n",
       "   'Em entrevista ao veículo , o responsável pelo estudo , que já que foi economista-chefe da Febraban , disse que em 2014 o Bradesco , o Itaú , o Santander , o Banco do Brasil e a Caixa Econômica Federal eram responsáveis por 77,3 % de todas as operações financeiras realizadas no Brasil .',\n",
       "   \"As análises feitas com diferentes genes sequenciados até o momento indicam que esse não parece ser um recombinante', disse à Folha a médica Teresa Peret, dos CDC (Centros para Controle e Prevenção de Doenças) dos EUA, uma brasileira que integrou a equipe que identificou o vírus.\",\n",
       "   'Uma infinidade de novas empresas de tecnologia financeira , incluindo as britânicas Revolut e Monzo , e a N26 , da Alemanha , seduziram milhões de clientes para suas plataformas .',\n",
       "   'A startup argentina Ripio , carteira digital mobile para serviços financeiros que possui operação no Brasil , e fechou uma parceria com o Mercado Pago , empresa de tecnologia de serviços de pagamentos do grupo Mercado Livre .'),\n",
       "  ('Marisa',\n",
       "   '22.ªCompanhia de Atiradores Pára-Quedistas',\n",
       "   'Itaú',\n",
       "   'Einstein',\n",
       "   'B2W',\n",
       "   'Francis WURTZ',\n",
       "   'Instituto de Física',\n",
       "   'ex-Bank of Tokyo Mitsubishi UFJ',\n",
       "   'Universidade de Minnesota',\n",
       "   'Governo',\n",
       "   'Uber',\n",
       "   'APSS',\n",
       "   'Santander',\n",
       "   'Centros para Controle e Prevenção de Doenças',\n",
       "   'N26',\n",
       "   'Mercado Pago'),\n",
       "  ('País',\n",
       "   'Brigada de Reacção Rápida',\n",
       "   'Ermírio de Moraes',\n",
       "   'Salesforce',\n",
       "   'Americanas.com',\n",
       "   'CEUE/EVN',\n",
       "   'Fapesp',\n",
       "   ' Brasil',\n",
       "   'Greg Laden',\n",
       "   'Assembleia Nacional Popular',\n",
       "   'Neon\"',\n",
       "   'Setúbal',\n",
       "   'Caixa Econômica Federal',\n",
       "   'EUA',\n",
       "   'Alemanha',\n",
       "   ' Ripio'),\n",
       "  ('“A parceria reforça nosso posicionamento como a maior empresa B2B white label da America Latina , provendo soluções e serviços end to end no conceito open banking voltados a contas digitais” , afirma Alexandre Brito , CEO da Hub Fintech , sem economizar anglicismos .',\n",
       "   'Igualmente em essa conferência, Portugal conseguiu da Espanha o reconhecimento de seus direitos sobre a colônia do Sacramento.',\n",
       "   'A cimeira União Europeia-África, prevista para 8 e 9 de Dezembro em Lisboa, não deve ser uma cimeira sobre o Presidente do Zimbabwe, Robert Mugabe, defendeu hoje José Sócrates, primeiro-ministro português e presidente em exercício da União Europeia (UE).',\n",
       "   'O AgFinTech – A Nova Era do Seguro e Crédito Rural vai reunir , na Casa do Saber , em São Paulo , especialistas de instituições financeiras , além de representante do Banco Central , para falar sobre temas como a Resolução 4.42 7 do BACEN , que permite a utilização do sensoriamento remoto para fins de fiscalização de operações de crédito rural .',\n",
       "   \"O movimento é mais um capítulo da 'guerra das maquininhas' , jargão do mercado para a crescente agressividade no mercado de adquirência de cartões , que deu uma guinada na semana passada após a Rede , do Itaú Unibanco , ter anunciado que zerou as taxas para antecipação de recebíveis de lojistas .\",\n",
       "   'mLabs recebe aporte da DOMO Invest e vai ampliar oferta de marketing digital para PMEs .',\n",
       "   'A RecargaPay , carteira de pagamentos digitais líder do Brasil , anuncia parceria com a Mastercard para oferecer aos seus 10 milhões de clientes o mais novo serviço da fintech : o cartão pré-pago .',\n",
       "   'O AgFinTech – A Nova Era do Seguro e Crédito Rural vai reunir , na Casa do Saber , em São Paulo , especialistas de instituições financeiras , além de representante do Banco Central , para falar sobre temas como a Resolução 4.42 7 do BACEN , que permite a utilização do sensoriamento remoto para fins de fiscalização de operações de crédito rural .',\n",
       "   'O banco CBSS , controlado pelos sócios Bradesco e Banco do Brasil , fez uma parceria com a fintech Geru para empréstimos a pessoas físicas .',\n",
       "   'Na educação , o Ministério planeja inserir , em parceria com o MEC , matérias sobre inovação nas universidades .',\n",
       "   'A inovação tecnológica tem levado os bancos a digitalizar serviços , reduzindo os custos , e a simplificar o acesso ao mercado e à informação , disse nesta quarta (27) , em Brasília , o presidente do Banco Central (BC) , Roberto Campos Neto , na abertura LiftDay , evento realizado pela autarquia em parceria com a Federação Nacional de Associações dos Servidores do Banco Central (Fenasbac) .',\n",
       "   'A expectativa é de chegar ao fim deste ano tendo transacionado um valor acumulado de R$ 1 bilhão , diz o CEO e fundador da PayMee , Bruno Maranhão .',\n",
       "   'Em 1917 entra para o Governo populista de Sidónio Pais, exercendo os cargos Ministro dos Negócios Estrangeiros, embaixador de Portugal em Espanha, e depois o de presidente da delegação portuguesa à Conferência de Paz, em Paris, no final da I Guerra Mundial (1914-1918).',\n",
       "   'O Cubo Itaú , espaço de fomento ao empreendedorismo tecnológico do Itaú Unibanco junto à Redpoint eventures , acaba de firmar parceria com Business Lab , da montadora francesa Groupe PSA , dona das marcas de veículos Peugeot e Citroën .',\n",
       "   'A fintech brasileira PagBrasil , que oferece processamento de pagamentos online no Brasil para comerciantes nacionais e internacionais .',\n",
       "   'O Maldi feito pelo grupo do LIP (Laboratório de Instrumentação e Partículas), do Instituto de Física, também financiado pela Fapesp, saiu bem mais em conta:'),\n",
       "  ('Hub Fintech',\n",
       "   'Portugal',\n",
       "   'José Sócrates',\n",
       "   'AgFinTech',\n",
       "   'Rede',\n",
       "   'DOMO',\n",
       "   'RecargaPay',\n",
       "   'AgFinTech',\n",
       "   'CBSS',\n",
       "   'Ministério',\n",
       "   'Banco Central',\n",
       "   'CEO',\n",
       "   'Governo populista',\n",
       "   'Redpoint',\n",
       "   'PagBrasil',\n",
       "   'Laboratório de Instrumentação e Partículas'),\n",
       "  (' Alexandre Brito',\n",
       "   'Sacramento',\n",
       "   'União Europeia',\n",
       "   'São Paulo',\n",
       "   'Itaú Unibanco',\n",
       "   'Invest',\n",
       "   ' Brasil',\n",
       "   'Banco Central',\n",
       "   'Geru',\n",
       "   'MEC',\n",
       "   ' Federação Nacional de Associações dos Servidores do Banco Central',\n",
       "   'Bruno Maranhão',\n",
       "   'Sidónio Pais',\n",
       "   ' Groupe PSA',\n",
       "   'Brasil',\n",
       "   'Instituto de Física'),\n",
       "  (\"A Resistência Islâmica, ala armada do Hizbollah, apoiada pelo Irão e pela Síria, reivindicou o ataque em um comunicado em que disse que visara 'uma coluna de líderes israelitas'.\",\n",
       "   'A abertura do evento contou com a presença de Fernando Gomes, presidente da Câmara Municipal do Porto, e do vereador Wan Chun, em representação de Sales Marques, presidente do Leal Senado, a qual foi enriquecida com um pequeno concerto em que os instrumentistas Wong On Yuen e António Ferro, tocando ehru e viola-baixo, respectivamente, interpretaram várias peças editadas no disco compacto Sinais de Yuanju.',\n",
       "   'O Samsung Pay anunciou segunda-feira (25 ) que a Trigg é a primeira fintech a juntar-se ao serviço de pagamento móvel no Brasil .',\n",
       "   'Há um muito parecido do Norte e outro no Alentejo, mas nada que se compare ao CEC em termos de estrutura organizacional e na representatividade.',\n",
       "   'Em os anos lionísticos de 88/89 e 90/91, foram angariados fundos para o Instituto D.Francisco Gomes, Casa dos Rapazes e, a partir de 1991 a Associação Portuguesa de Paralisia Cerebral tem sido alvo de múltiplas campanhas, tendo em vista a construção da sua sede.',\n",
       "   \"A Legião da Boa Vontade, instituição educacional, cultural, beneficente e filantrópica, foi fundada no Brasil pelo jornalista, radialista e poeta Alziro Zarur no programa de rádio 'Hora da Boa Vontade', na Rádio Globo do Rio de Janeiro.\",\n",
       "   'Opinião idêntica tem o gestor do Banif Gestão de Activos, Pedro Castro, para quem a expectativa é que se estreiem na bolsa portuguesa, além da ANA e da TAP, também a Aquapor, a Sonae Capital e a Neo.',\n",
       "   'O Grupo Carrefour Brasil informou nesta segunda-feira que o Banco Carrefour lançou um marketplace de seguros e serviços , destinado a clientes do cartão Carrefour e do cartão Atacadão , que terão condições especiais , e não clientes que possuem outros cartões de crédito .',\n",
       "   'MinTech : a fintech que nasceu nos EUA , cresceu na China e chegou ao Brasil .',\n",
       "   'A fim de evitar que seus clientes paguem pelas transferências via TED ou DOC , a fintech disponibiliza contas nos bancos Itaú , Santander , Bradesco , Banco do Brasil e Banco Inter e Caixa Econômica , para onde o investidor envia o valor desejado .',\n",
       "   'O Hamas a mandar em Gaza, e a OLP a fingir que manda na Cisjordânia.',\n",
       "   'A estimativa foi apresentada por John Schellnhuber, do Centro Tyndall em Norwich, em uma conferência sobre os efeitos da mudança climática durante a reunião da AAAS.',\n",
       "   'Segundo a Uber , a parceria cria uma ferramenta chamada U-Check , que foi desenvolvida pelo Centro de Desenvolvimento Tecnológico (Tech Center) em parceria com o time de engenheiros da Uber em San Francisco (EUA) .',\n",
       "   'O Hurb , novo nome do Hotel Urbano e o Santander Esfera , programa de fidelidade do Banco Santander , firmaram uma parceria que vai render pontos e vantagens para os clientes de ambas as empresas .',\n",
       "   'Forças do BC9 de Viana do Castelo tomam o Aeroporto de Pedras Rubras.',\n",
       "   'Em entrevista ao veículo , o responsável pelo estudo , que já que foi economista-chefe da Febraban , disse que em 2014 o Bradesco , o Itaú , o Santander , o Banco do Brasil e a Caixa Econômica Federal eram responsáveis por 77,3 % de todas as operações financeiras realizadas no Brasil .'),\n",
       "  ('Resistência Islâmica',\n",
       "   'Câmara Municipal do Porto',\n",
       "   'Trigg',\n",
       "   'Norte',\n",
       "   'Casa dos Rapazes',\n",
       "   'Legião da Boa Vontade',\n",
       "   'Aquapor',\n",
       "   'Grupo Carrefour Brasil',\n",
       "   'MinTech',\n",
       "   'Banco do Brasil',\n",
       "   'OLP',\n",
       "   'AAAS',\n",
       "   'Centro de Desenvolvimento Tecnológico (Tech Center)',\n",
       "   'Hurb',\n",
       "   'BC9',\n",
       "   'Febraban'),\n",
       "  ('Síria',\n",
       "   'Wan Chun',\n",
       "   'Samsung Pay',\n",
       "   'CEC',\n",
       "   'Associação Portuguesa de Paralisia Cerebral',\n",
       "   'Hora da Boa Vontade',\n",
       "   'Sonae Capital',\n",
       "   'Carrefour',\n",
       "   'Brasil',\n",
       "   'Caixa Econômica',\n",
       "   'Cisjordânia',\n",
       "   'John Schellnhuber',\n",
       "   ' San Francisco',\n",
       "   ' Hotel Urbano',\n",
       "   'Aeroporto de Pedras Rubras',\n",
       "   'Itaú'),\n",
       "  ('A identificação e descrição está sendo feita com a ajuda de Marina Capelari, do Instituto de Botânica de São Paulo, e Dennis Desjardin, da Universidade Estadual de San Francisco (Estados Unidos).',\n",
       "   \"Se eles olhassem como ficou a lata[ diferente da criação original], não haveria problema legal ', afirmou Jakob Fenger, um dos membros do coletivo, em entrevista para a Folha de São Paulo, em matéria que informa ainda:' Por meio de sua assessoria, a AmBev[ fabricante do guaraná Antarctica] declarou não ver problema na obra.\",\n",
       "   'Retorna a os Estados Unidos onde está morando, para fazer a campanha da Gap.',\n",
       "   'É o que revela balanço do registro de imóveis divulgado nesta segunda-feira (30 ) pela Associação dos Registradores de Imóveis de São Paulo (Arisp) em parceria com a Fundação Instituto de Pesquisas Econômicas (Fipe) .',\n",
       "   'O Samsung Creative Startups , programa de aceleração de empreendimentos de base tecnológica da empresa em parceria com a Anprotec (Associação Nacional de Entidades Promotoras de Empreendimentos Inovadores) e o CCEI (Centro Coreano de Economia Criativa e Inovação) , acaba de divulgar as 31 startups pré-selecionadas para a 4ª Rodada do programa .',\n",
       "   \"Em oito anos, detectamos mais de 300 eventos, graças ao o nosso sistema de calibragem dos dados de satélite', conta Douglas Revelle, do Laboratório Nacional de Los Alamos, um dos autores do estudo, que está publicado na edição de hoje da revista britânica 'Nature class'entity'' (www.nature.com).\",\n",
       "   'O Cubo Itaú , espaço de fomento ao empreendedorismo tecnológico do Itaú Unibanco junto à Redpoint eventures , acaba de firmar parceria com Business Lab , da montadora francesa Groupe PSA , dona das marcas de veículos Peugeot e Citroën .',\n",
       "   'Sete ativistas do Greenpeace foram presos ontem em os EUA ao tentar impedir o descarregamento de madeira brasileira na cidade de Savannah, na costa do Estado da Geórgia.',\n",
       "   'Essa foi uma das principais constatações da pesquisa “Financiamento dos Pequenos Negócios 2018” , realizada entre os meses de junho e agosto pelo Sebrae e apresentada na abertura do Fórum de Cidadania Financeira , em Brasília (DF) .',\n",
       "   'O InsureTech Connect (ITC) , o maior evento de tecnologia de seguros , terá uma atração a mais este ano em parceria com o Insurance Information Institute (III) , foi lançada uma competição destinada a startups que trabalham em soluções para as preocupações voltadas aos desastres naturais .',\n",
       "   'A Apple deve lançar seu próprio cartão de crédito em parceria com o banco Goldman Sach .',\n",
       "   'Essa conclusão veio de um estudo feito por Domingos Matos, 36, médico da Universidade Federal do Pará.',\n",
       "   '\"N o primeiro momento , o app irá destacar os pagamentos feitos a Netflix , iFood , Spotify , Uber e PayPal , os serviços digitais mais usados pelos clientes na plataforma da Neon\" , afirma a fintech em nota .',\n",
       "   'Estamos transferindo esses ganhos aos nossos clientes , disse Pedro Coutinho , CEO da Getnet , ao Valor , do escritório da companhia em São Paulo .',\n",
       "   'A questão é que boa parte do dinheiro ainda está nos bancos , bandeiras e credenciadoras , que é um sistema mais caro para os varejistas , diz Tulio Oliveira , diretor do Mercado Pago no Brasil , conta digital do Mercado Livre .',\n",
       "   \"Para Greg Laden, da Universidade de Minnesota, e Richard Wrangham, da Universidade Harvard, raízes, tubérculos e outros USOs (sigla em inglês para 'órgãos subterrâneos de armazenamento' das plantas) permitiram que os primeiros hominídeos, como são conhecidos os ancestrais do homem, saíssem da mata fechada e colonizassem ambientes abertos a partir de uns 6 milhões de anos atrás.\"),\n",
       "  ('Instituto de Botânica de São Paulo',\n",
       "   'Jakob Fenger',\n",
       "   'Estados Unidos',\n",
       "   'Associação dos Registradores de Imóveis de São Paulo (Arisp)',\n",
       "   'Samsung Creative Startups',\n",
       "   'Laboratório Nacional de Los Alamos',\n",
       "   'Business Lab',\n",
       "   'Greenpeace',\n",
       "   'Sebrae',\n",
       "   'InsureTech Connect',\n",
       "   'Apple',\n",
       "   'Universidade Federal do Pará',\n",
       "   'iFood',\n",
       "   'Getnet',\n",
       "   'Mercado Pago',\n",
       "   'Universidade de Minnesota'),\n",
       "  ('Dennis Desjardin',\n",
       "   'Folha de São Paulo',\n",
       "   'Gap',\n",
       "   ' Fundação Instituto de Pesquisas Econômicas (Fipe)',\n",
       "   'Associação Nacional de Entidades Promotoras de Empreendimentos Inovadores',\n",
       "   'Douglas Revelle',\n",
       "   'Citroën',\n",
       "   'Estado da Geórgia',\n",
       "   'Brasília',\n",
       "   'Insurance Information Institute (III)',\n",
       "   'Goldman Sach',\n",
       "   'Domingos Matos',\n",
       "   'Neon\"',\n",
       "   ' Pedro Coutinho',\n",
       "   ' Brasil',\n",
       "   'Universidade Harvard'),\n",
       "  ('A nossa Escola de Pilotagem ajudou a formar dezenas de pilotos particulares e profissionais, sendo actualmente dirigida pelo Cmdt. João Filhó, um profissional da aviação que trabalha na Portugália, e que é um entre as dezenas de profissionais que iniciaram ou reforçaram a sua actividade de pilotos no ACTV.',\n",
       "   'Agtech israelense See Tree recebe apoio para atuar no Brasil.N o final de abril , estivemos na Agrishow para conhecer as startups presentes na Arena da Inovação e falamos sobre a SeeTree , uma agtech israelense ainda pouco conhecida , mas que já atuava no Brasil com sua solução de monitoramento de cítricos .',\n",
       "   'Para oferecer os serviços financeiros , a Avec faz parcerias com outras empresas , que desenvolvem as ferramentas desejadas , como Zoop e VTEX .',\n",
       "   'Mistura entre fintech e empresa nutricional , a Nutrebem atua em 200 escolas , a maior parte em São Paulo e no Rio , onde foi fundada .',\n",
       "   'Os clientes podem ganhar até 50% de desconto em lojas parceiras , como , por exemplo , Netshoes , Zattini , Beer.com.br , Vinho Fácil , entre outras , Além das parcerias , a fintech também aumentará o desconto oferecido para quem antecipar suas compras parceladas , podendo chegar até 5% do valor do produto para quem comprar em 12x .',\n",
       "   'A equipe de Peter Savolainen, do Instituto Real de Tecnologia, de Estocolmo, Suécia, constatou que a diversidade genética era maior entre os cães do leste asiático.',\n",
       "   \"Em testes in vitro, elas foram eficientes contra o câncer de mama, por exemplo', afirma Jonas Perales, do Laboratório de Toxinologia, que coordena os estudos.\",\n",
       "   'o Banco Original começará a divulgar uma série de parcerias com fintechs , segundo fontes ouvidas pelo GLOBO .',\n",
       "   'A fim de evitar que seus clientes paguem pelas transferências via TED ou DOC , a fintech disponibiliza contas nos bancos Itaú , Santander , Bradesco , Banco do Brasil e Banco Inter e Caixa Econômica , para onde o investidor envia o valor desejado .',\n",
       "   'Realizado no Lounge Plant , o debate contou com a participação de Percival Lucena , pesquisador da IBM , Fernando Martins , conselheiro da Jacto e de startups AgTech , e Rodrigo Iafelice dos Santos , presidente da Ennexas Agribusiness .',\n",
       "   'André Boaventura , sócio e agora CMO do Ebanx , fala sobre o impacto de ser uma empresa unicórnio para o marketing .',\n",
       "   \"É também o Quénia, 'ao lado da Nigéria, que se incendeia', diz Moncef Kaabi, da Natixis, é também a Turquia, com as operações contra os separatistas curdos no Norte, justificando que mercados como o do petróleo 'traduzem a inquietação e a incerteza dos investidores'.\",\n",
       "   'As empresas selecionadas também terão direito à estrutura do WeWork da Av. Brig.',\n",
       "   'A rede de franquias de pagamentos eletrônicos Acqio , em parceria com a bandeira Elo e a Cateno , especializada em soluções de meios de pagamento , lança o cartão pré-pago Acqio para franqueados responsáveis pelas vendas de suas maquininhas .',\n",
       "   'A Embrapa Café e o Consórcio Pesquisa Café , em parceria com a Universidade Federal de Lavras (UFLA ) e a Universidade Federal de Viçosa (UFV) , selecionaram 26 startups para participarem da etapa piloto do projeto para a pré-aceleração de startups – Avança Café .',\n",
       "   'A Adyen , processadora de pagamentos pioneira na adoção das carteiras digitais no Brasil , se uniu ao Google para integrar a função débito ao Google Pay .'),\n",
       "  ('Escola de Pilotagem',\n",
       "   'SeeTree',\n",
       "   'Avec',\n",
       "   'Nutrebem',\n",
       "   'Netshoes',\n",
       "   'Instituto Real de Tecnologia',\n",
       "   'Laboratório de Toxinologia',\n",
       "   'Banco Original',\n",
       "   'Itaú',\n",
       "   'Jacto',\n",
       "   'André Boaventura ',\n",
       "   'Moncef Kaabi',\n",
       "   'WeWork',\n",
       "   'Elo',\n",
       "   'Embrapa Café',\n",
       "   'Google'),\n",
       "  ('Portugália',\n",
       "   'Brasil',\n",
       "   ' Zoop',\n",
       "   'Rio',\n",
       "   'Beer.com.br',\n",
       "   'Estocolmo',\n",
       "   'Jonas Perales',\n",
       "   ' GLOBO',\n",
       "   'Banco Inter',\n",
       "   'Percival Lucena',\n",
       "   ' Ebanx',\n",
       "   'Natixis',\n",
       "   ' Av',\n",
       "   'Cateno',\n",
       "   ' Universidade Federal de Viçosa (UFV',\n",
       "   ' Brasil'),\n",
       "  ('O “Sebrae like a Farmer” faz parte da programação do Agrobit Brasil , evento criado para integrar produtores e empresários do agronegócio com soluções tecnológicas e inovadoras disponíveis e capazes de contribuir com o desafio de aumentar a produtividade e rentabilidade do agronegócio .',\n",
       "   'Sem o PSD, na fórmula da AD de Sá Carneiro, não se iria tão longe e tão cedo no fim da presença militar na tutela da democracia.',\n",
       "   \"Para Greg Laden, da Universidade de Minnesota, e Richard Wrangham, da Universidade Harvard, raízes, tubérculos e outros USOs (sigla em inglês para 'órgãos subterrâneos de armazenamento' das plantas) permitiram que os primeiros hominídeos, como são conhecidos os ancestrais do homem, saíssem da mata fechada e colonizassem ambientes abertos a partir de uns 6 milhões de anos atrás.\",\n",
       "   '“As iniciativas estão sendo feitas em parceria com corretores de seguros , startups , insurtechs e organizações de diferentes setores” , diz Laercio Cerboncini , responsável pela área digital da Chubb no Brasil .',\n",
       "   'Agtech israelense See Tree recebe apoio para atuar no Brasil.N o final de abril , estivemos na Agrishow para conhecer as startups presentes na Arena da Inovação e falamos sobre a SeeTree , uma agtech israelense ainda pouco conhecida , mas que já atuava no Brasil com sua solução de monitoramento de cítricos .',\n",
       "   'O Banco Inter não é aceito por algumas empresas como o Google Adsense , Skrill e Neteller .',\n",
       "   'DRV: Qual é o peso que a região de Viseu tem, a nível empresarial, no CEC.',\n",
       "   'Nossa meta é alcançar 10 mil empréstimos por mês” , contou Mellissa Penteado , CEO do Bancoin .',\n",
       "   'A empresa de maquinhas de cartão Stone anunciou que fechou uma parceria com o Grupo Globo .',\n",
       "   'A Hub Fintech , empresa de meios de pagamento que tem o bilionário Carlos Wizard Martins como dono de 90% do capital , acaba de fechar um contrato com a Visa , maior companhia de pagamentos digitais no mundo .',\n",
       "   'O sistema dispensa fiador e tem taxas mais baixas do que as cobradas por seguradoras” , explica Fábio Cruz , CTO da CredPago .',\n",
       "   'DRV: Quais as propostas do CEC, durante este seu mandato, até 2002?',\n",
       "   'Até o final deste ano , o Cilia espera chegar a um milhão de orçamentos , com a ajuda do Fasttrack .',\n",
       "   'Bradesco utilizará a tecnologia da Ripple para realizar transferências internacionais para o Japão , numa parceria com o MUFG Bank , ex-Bank of Tokyo Mitsubishi UFJ .',\n",
       "   'Cleo irá realizar serviços como consultas de saldos e limites , descrição de compras realizadas no cartão , emissão de 2ª via de boleto , parcelamento de fatura , entre outros A administradora de cartões de crédito para o mercado varejista , Cred-System , firmou parceria com a Neobpo , empresa especializada na oferta de soluções , tecnologia e serviços de BPO , para desenvolver um Assistente Virtual Inteligente',\n",
       "   'A Embrapa Café e o Consórcio Pesquisa Café , em parceria com a Universidade Federal de Lavras (UFLA) e a Universidade Federal de Viçosa (UFV) , selecionaram 26 startups para participarem da etapa piloto do projeto para a pré-aceleração de startups – Avança Café .'),\n",
       "  ('“Sebrae like a Farmer”',\n",
       "   'PSD',\n",
       "   'Universidade de Minnesota',\n",
       "   'Chubb',\n",
       "   'See Tree',\n",
       "   'Google Adsense',\n",
       "   'Viseu',\n",
       "   'Bancoin',\n",
       "   'Grupo Globo',\n",
       "   'Hub Fintech',\n",
       "   'CredPago',\n",
       "   'DRV',\n",
       "   'Fasttrack',\n",
       "   'Ripple',\n",
       "   'BPO',\n",
       "   'Consórcio Pesquisa Café'),\n",
       "  ('Agrobit Brasil',\n",
       "   'AD',\n",
       "   'Richard Wrangham',\n",
       "   ' Brasil',\n",
       "   'SeeTree',\n",
       "   'Skrill',\n",
       "   'CEC',\n",
       "   'Mellissa Penteado',\n",
       "   ' Stone',\n",
       "   'Carlos Wizard Martins',\n",
       "   'Fábio Cruz',\n",
       "   'CEC',\n",
       "   'Cilia',\n",
       "   ' ex-Bank of Tokyo Mitsubishi UFJ',\n",
       "   ' Cleo',\n",
       "   'Universidade Federal de Lavras'),\n",
       "  ('A Indigo também chega ao País oferecendo sua tecnologia de blockchain para investidores fecharem contratos de barter – a tecnologia permite que os acordos sejam menos burocráticos e mais seguros .',\n",
       "   \"Kevin Hale, o responsável de segurança da NAS as, disse-me que era porque alguém havia exprimido preocupação '.\",\n",
       "   \"Esses corpos medidos em metros são interessantes cientificamente, mas não oferecem absolutamente nenhum perigo a os humanos', diz Robert Jedicke, da Universidade do Arizona, escolhido pela 'Nature' para comentar o estudo.\",\n",
       "   'DRV: Quantas empresas representa o CEC?',\n",
       "   'O Cubo Itaú , espaço de fomento ao empreendedorismo tecnológico do Itaú Unibanco junto à Redpoint eventures , acaba de firmar parceria com Business Lab , da montadora francesa Groupe PSA , dona das marcas de veículos Peugeot e Citroën .',\n",
       "   'No fim de janeiro , Corinthians anunciou o Banco BMG como parceiro .',\n",
       "   'Realizado no Lounge Plant , o debate contou com a participação de Percival Lucena , pesquisador da IBM , Fernando Martins , conselheiro da Jacto e de startups AgTech , e Rodrigo Iafelice dos Santos , presidente da Ennexas Agribusiness .',\n",
       "   '\"N o primeiro momento , o app irá destacar os pagamentos feitos a Netflix , iFood , Spotify , Uber e PayPal , os serviços digitais mais usados pelos clientes na plataforma da Neon\" , afirma a fintech em nota .',\n",
       "   \"Esse é um alvo viável para remédios contra a obesidade', disse um dos autores, John Clapham, da empresa farmacêutica SmithKline Beecham, que fez o estudo em colaboração com a Universidade de Cambridge, Reino Unido.\",\n",
       "   'O Samsung Creative Startups , programa de aceleração de empreendimentos de base tecnológica da empresa em parceria com a Anprotec (Associação Nacional de Entidades Promotoras de Empreendimentos Inovadores) e o CCEI (Centro Coreano de Economia Criativa e Inovação) , acaba de divulgar as 31 startups pré-selecionadas para a 4ª Rodada do programa .',\n",
       "   'De acordo com a CBKC (Confederação Brasileira de Cinofilia), órgão filiado ao FCI (Fédération Cynologique Internationale), existem onze grupos de raças no Brasil.',\n",
       "   'A inovação está presente no DNA da Mastercard e do C6 Bank e , cada vez mais , temos visto essa parceria se transformar em soluções práticas e vantajosas para nossos clientes” , diz Maria Cecilia Machado e Silva , head de cartões no C6 Bank .',\n",
       "   'Os clientes podem ganhar até 50% de desconto em lojas parceiras , como , por exemplo , Netshoes , Zattini , Beer.com.br , Vinho Fácil , entre outras , Além das parcerias , a fintech também aumentará o desconto oferecido para quem antecipar suas compras parceladas , podendo chegar até 5% do valor do produto para quem comprar em 12x .',\n",
       "   'Agtech israelense See Tree recebe apoio para atuar no Brasil.N o final de abril , estivemos na Agrishow para conhecer as startups presentes na Arena da Inovação e falamos sobre a SeeTree , uma agtech israelense ainda pouco conhecida , mas que já atuava no Brasil com sua solução de monitoramento de cítricos .',\n",
       "   'O Cubo Itaú , espaço de fomento ao empreendedorismo tecnológico do Itaú Unibanco junto à Redpoint eventures , acaba de firmar parceria com Business Lab , da montadora francesa Groupe PSA , dona das marcas de veículos Peugeot e Citroën .',\n",
       "   'A B2W , dona da Americanas.com e do Submarino , estuda separar o braço da Ame Digital de suas operações .'),\n",
       "  ('Indigo',\n",
       "   'Kevin Hale',\n",
       "   'Universidade do Arizona',\n",
       "   'DRV',\n",
       "   'Itaú Unibanco',\n",
       "   'Corinthians',\n",
       "   'Jacto',\n",
       "   'Netflix',\n",
       "   'SmithKline Beecham',\n",
       "   'Samsung Creative Startups',\n",
       "   'CBKC',\n",
       "   'C6 Bank',\n",
       "   'Beer.com.br',\n",
       "   'Agtech',\n",
       "   'Groupe PSA',\n",
       "   'B2W'),\n",
       "  (' País',\n",
       "   'NAS',\n",
       "   'Robert Jedicke',\n",
       "   'CEC',\n",
       "   ' Citroën',\n",
       "   ' Banco BMG',\n",
       "   'Ennexas Agribusiness',\n",
       "   'iFood',\n",
       "   'Reino Unido',\n",
       "   'Anprotec',\n",
       "   'FCI',\n",
       "   ' Maria Cecilia Machado e Silva',\n",
       "   'Vinho Fácil',\n",
       "   'Brasil',\n",
       "   ' Citroën',\n",
       "   ' Americanas.com'),\n",
       "  ('O Grupo Pão de Açúcar (GPA) anunciará nos próximos meses uma parceria com o Itaú Unibanco para oferecer serviços de meios de pagamento , disse Antonio Salvador , diretor de transformação digital .',\n",
       "   'Nuno Severiano Teixeira reúne-se ainda com o comandante da força da Nato (ISAF) no Afeganistão, General Dan McNeill.',\n",
       "   'Realizado no Lounge Plant , o debate contou com a participação de Percival Lucena , pesquisador da IBM , Fernando Martins , conselheiro da Jacto e de startups AgTech , e Rodrigo Iafelice dos Santos , presidente da Ennexas Agribusiness .',\n",
       "   'O banQi , novo banco digital que surgiu da aliança entre Via Varejo , controladora da Casas Bahia , e a fintech Airfox , anuncia novas parcerias com Mastercard , Zurich e Cielo que disponibilizarão novas soluções financeiras , formas de pagamento e a venda de seguros massificados via app .',\n",
       "   'Além da ferramenta , que está disponível no site BB Startups , os empreendedores encontrarão detalhes sobre as áreas de inovação da instituição financeira. São elas os laboratórios de inovação do BB no Brasil e no Vale do Silício.',\n",
       "   'Os clientes podem ganhar até 50% de desconto em lojas parceiras , como , por exemplo , Netshoes , Zattini , Beer.com.br , Vinho Fácil , entre outras , Além das parcerias , a fintech também aumentará o desconto oferecido para quem antecipar suas compras parceladas , podendo chegar até 5% do valor do produto para quem comprar em 12x .',\n",
       "   'México prende assessor de deputado do PRI das agências internacionais A polícia mexicana prendeu ontem um funcionário da Câmara por suspeita de envolvimento no assassinato do secretário-geral do Partido Revolucionário Institucional (PRI, governista), José Francisco Ruiz Massieu, 48, ocorrido quarta-feira na Cidade do México.',\n",
       "   'Eu não posso deixar de louvar a atitude de V.Exa., prestando assim esses informes à Casa, de forma tão clara e insofismável.',\n",
       "   'O Grupo Carrefour Brasil informou nesta segunda-feira que o Banco Carrefour lançou um marketplace de seguros e serviços , destinado a clientes do cartão Carrefour e do cartão Atacadão , que terão condições especiais , e não clientes que possuem outros cartões de crédito .',\n",
       "   'Essas operações passam a ser possíveis a partir de parceria da ABFintechs (associação que reúne as startups do setor financeiro ) com a empresa Saque e Pague , que possui rede de 1.1 0 caixas de autoatendimento .',\n",
       "   'O Sicoob Central SC/RS promoveu no Attive Coworking , em Porto Alegre (RS) , um Workshop de Inovação nos dias 7 e 8 de junho .',\n",
       "   \"Manifestando-se 'muito honrado como europeu e como português' por participar na cerimónia de proclamação da Carta dos Direitos Fundamentais da UE - que definiu como 'a mais importante de toda a carreira política' - SÓCRATES salientou ainda o compromisso assumido pelas três instituições - PE, Conselho e Comissão - de que 'os valores fundamentais serão respeitados e aplicados'.\",\n",
       "   'Em entrevista ao veículo , o responsável pelo estudo , que já que foi economista-chefe da Febraban , disse que em 2014 o Bradesco , o Itaú , o Santander , o Banco do Brasil e a Caixa Econômica Federal eram responsáveis por 77,3 % de todas as operações financeiras realizadas no Brasil .',\n",
       "   'Igualmente em essa conferência, Portugal conseguiu da Espanha o reconhecimento de seus direitos sobre a colônia do Sacramento.',\n",
       "   \"Estamos recolhendo amostras de coprólitos [fezes fossilizadas] de duas cavernas em Israel com 40 mil anos, onde provavelmente Cro-Magnons [os primeiros humanos modernos] e neandertais viveram lado a lado', contou o pesquisador durante a reunião da AAAS (Associação Americana para o Avanço da Ciência).\",\n",
       "   'Temendo que a Nasa tenha de aposentar seus ônibus espaciais antes mesmo de 2010, a ESA (Agência Espacial Européia) já está estudando o que fazer caso seu módulo não possa ir ao espaço.'),\n",
       "  ('Grupo Pão de Açúcar (GPA)',\n",
       "   'Nato',\n",
       "   'IBM',\n",
       "   'banQI',\n",
       "   'BB',\n",
       "   'Zattini',\n",
       "   'Câmara',\n",
       "   'V.Exa.',\n",
       "   'Banco Carrefour',\n",
       "   'ABFintechs',\n",
       "   'Sicoob Central SC/RS',\n",
       "   'SÓCRATES',\n",
       "   'Santander',\n",
       "   'Portugal',\n",
       "   'AAAS',\n",
       "   'Nasa'),\n",
       "  (' Itaú Unibanco',\n",
       "   'Afeganistão',\n",
       "   'Ennexas Agribusiness',\n",
       "   'Zurich',\n",
       "   ' Brasil',\n",
       "   'Beer.com.br',\n",
       "   'Partido Revolucionário Institucional',\n",
       "   'Casa',\n",
       "   ' Atacadão',\n",
       "   ' Saque e Pague',\n",
       "   ' Attive Coworking',\n",
       "   'Comissão',\n",
       "   'Brasil',\n",
       "   'Espanha',\n",
       "   'Israel',\n",
       "   'ESA'),\n",
       "  ('A Mastercard fará uma parceria com a fintech Token para desenvolver um hub de Open Banking .',\n",
       "   'O Samsung Pay anunciou segunda-feira (25 ) que a Trigg é a primeira fintech a juntar-se ao serviço de pagamento móvel no Brasil .',\n",
       "   'A startup paulista começou seus trabalhos há pouco mais de 2 anos , e agora , com um produto SaaS (softwar e as a service ) sólido e implementado em grandes clientes como : Coca-Cola , Nestlé e Siemens , a Incentivar.i o considera o momento ideal para disponibilizar seu produto de incentivo para pequenas e médias empresas .',\n",
       "   'Além disso , a aceleradora trabalha junto com grandes empresas como o Hospital Einstein , o Banco do Brasil e a bandeira de cartões de crédito Elo .',\n",
       "   'O banQi , resultado da aliança entre Via Varejo , controladora da Casas Bahia , e a fintech Airfox , startup de soluções de pagamentos móveis e digitais , anuncia parcerias com Mastercard , Zurich e Cielo .',\n",
       "   'Esta é a segunda vez que a Solinftec , agtech líder em agricultura digital no Brasil , é indicada ao prêmio .',\n",
       "   'Para atender a essa demanda , a Federação Brasileira de Bancos (Febraban ) desenvolveu , em parceria com o Sebrae , materiais de estudo para a área que estão disponíveis no “Canal do Empreendedor” , no site de finanças pessoais.',\n",
       "   'Rodrigo Ventura decidiu criar a 88i após validar sua tese no Startup Weekend Blockchain Techstars , evento patrocinado pelo Google e pela TechStars , braço de inovação do banco Barclays da Inglaterra .',\n",
       "   'O Santander Gestão de Activos aponta para que as novas listagens no próximo ano na Euronext Lisboa aconteçam quer através de privatizações, dando como exemplos os casos da ANA e da TAP, quer através de sociedades de capital privado como o grupo Visabeira.',\n",
       "   'Com a proposta de fazer uso da tecnologia para inovar na gestão de recebíveis para agroindústrias , distribuidores de insumos agrícolas , cooperativas e tradings , a GIRA (Gestã o Integrada de Recebíveis do Agronégócio ) acaba de receber um aporte do fundo Venture Brasil Central , gerido pela Cedro Capital .',\n",
       "   'A ABES (Assciaçã o Brasileira de Empresas de Software ) fecha parceria com a Câmara de Comércio Brasil Canadá (CCBC) , organização independente que procura estreitar as relações bilaterais entre Brasil e Canadá , para atrair startups canadenses para o mercado brasileiro .',\n",
       "   'O Maldi feito pelo grupo do LIP (Laboratório de Instrumentação e Partículas), do Instituto de Física, também financiado pela Fapesp, saiu bem mais em conta:',\n",
       "   'Na educação , o Ministério planeja inserir , em parceria com o MEC , matérias sobre inovação nas universidades .',\n",
       "   'A Natura , por exemplo , fez parceria com o Santander para oferecer a 1 milhão de consultores os serviços de conta corrente digital sem custo , microcrédito , maquininha e cartão com funções crédito e débito .',\n",
       "   'A TrustHub , fintech do grupo SRM e que também tem uma gestora especializada em fundos de recebíveis , fechou acordo com o Grupo Ipiranga , para antecipação de recebíveis de seus clientes corporativos .',\n",
       "   'Microsoft e o Nubank anunciaram uma parceria inusitada permitindo aos usuários da Microsoft Store que “apaguem” seus gastos com Office , Xbox Live , jogos e outros produtos através do Nubank Rewards .'),\n",
       "  ('Mastercard',\n",
       "   'Trigg',\n",
       "   'Coca-Cola',\n",
       "   'Hospital Einstein',\n",
       "   'Casas Bahia',\n",
       "   'Solinftec',\n",
       "   'Federação Brasileira de Bancos (Febraban',\n",
       "   'TechStars',\n",
       "   'Santander Gestão de Activos',\n",
       "   'GIRA (Gestã o Integrada de Recebíveis do Agronégócio',\n",
       "   'ABES (Assciaçã o Brasileira de Empresas de Software',\n",
       "   'LIP',\n",
       "   'Ministério',\n",
       "   'Natura',\n",
       "   'TrustHub',\n",
       "   'Microsoft'),\n",
       "  (' Token',\n",
       "   'Brasil',\n",
       "   'Nestlé',\n",
       "   'Banco do Brasil',\n",
       "   ' Cielo',\n",
       "   'Brasil',\n",
       "   ' Sebrae',\n",
       "   'Barclays',\n",
       "   'Euronext Lisboa',\n",
       "   'Cedro Capital',\n",
       "   'Brasil',\n",
       "   'Instituto de Física',\n",
       "   ' MEC',\n",
       "   ' Santander',\n",
       "   'Grupo Ipiranga',\n",
       "   ' Nubank'),\n",
       "  ('Segundo a agência de notícias oficial Xinhua, o comitê central do Partido Comunista Chinês, após uma reunião, alertou as autoridades de saúde a não esconder casos da pneumonia, conhecida pela sigla Sars (síndrome respiratória aguda grave).',\n",
       "   'O banQi , resultado da aliança entre Via Varejo , controladora da Casas Bahia , e a fintech Airfox , startup de soluções de pagamentos móveis e digitais , anuncia parcerias com Mastercard , Zurich e Cielo .',\n",
       "   'Sem agências e com operação de baixo custo (mas não isenta de tarifas) , o C6 fechou ainda parcerias com laboratórios de pesquisa do MIT (Instituto de Tecnologia de Massachusetts) para buscar soluções inovadoras para problemas financeiros , um deles é a construção do modelo de crédito com base em dados alternativos , na tentativa de cobrar juros mais baixos .',\n",
       "   'Agtech israelense See Tree recebe apoio para atuar no Brasil.N o final de abril , estivemos na Agrishow para conhecer as startups presentes na Arena da Inovação e falamos sobre a SeeTree , uma agtech israelense ainda pouco conhecida , mas que já atuava no Brasil com sua solução de monitoramento de cítricos .',\n",
       "   \"Em os primeiros anos da democracia, este 'bloco central' concentrava-se em exercer o poder através da economia nacionalizada pós-1975, de aí poder ser representado simbolicamente na cena real de responsáveis dos dois partidos reunidos a distribuir os lugares de gestores públicos em as empresas: estes dois são para mim, este fica para ti, este vai para a TAP e dou-te dois na CP, mas este da EDP não pode ser comparado como este na Imprensa Nacional, etc..\",\n",
       "   \"Se eles olhassem como ficou a lata[ diferente da criação original], não haveria problema legal ', afirmou Jakob Fenger, um dos membros do coletivo, em entrevista para a Folha de São Paulo, em matéria que informa ainda:' Por meio de sua assessoria, a AmBev[ fabricante do guaraná Antarctica] declarou não ver problema na obra.\",\n",
       "   'O Banco Inter não é aceito por algumas empresas como o Google Adsense , Skrill e Neteller .',\n",
       "   'A inovação tecnológica tem levado os bancos a digitalizar serviços , reduzindo os custos , e a simplificar o acesso ao mercado e à informação , disse nesta quarta (27) , em Brasília , o presidente do Banco Central (BC) , Roberto Campos Neto , na abertura LiftDay , evento realizado pela autarquia em parceria com a Federação Nacional de Associações dos Servidores do Banco Central (Fenasbac) .',\n",
       "   'O achado foi apresentado por Mark Mattson, do Instituto Nacional do Envelhecimentos dos EUA, durante a 170ª Reunião Anual da AAAS (Associação Americana para o Avanço da Ciência), em Seattle, no noroeste dos Estados Unidos.',\n",
       "   'A Airfox , uma startup que fornece serviços financeiros transformadores para os mercados emergentes , anunciou hoje uma parceria com a ZestFinance , especialista em inteligência artificial , e a gigante do varejo brasileiro Via Varejo para entregar um novo modelo de microcrédito com infraestrutura algor .',\n",
       "   '“Estamos planejando reativar nossas próprias operações de crediário usando a infraestrutura digital do BanQi” , disse Klein , referindo-se ao banco digital que a Via Varejo lançou em junho por meio de uma parceria com a startup norte-americana Airfox .',\n",
       "   'O serviço que permite que empresas se comuniquem e interajam com clientes por meio do aplicativo de Mensagens tem seu primeiro parceiro oficializado no Brasil : a Porto Seguro Cartões , subsidiária do grupo Porto Seguro .',\n",
       "   \"Pesquisa de opinião publicada ontem pelos jornais 'Reforma', da capital, e 'El Norte', de Monterrey, diz que 29 $% dos entrevistados acreditam que Ruiz foi morto por ordem da 'velha guarda' do PRI (em o poder desde 1929).\",\n",
       "   'A Firgun trabalha em parceria com organizações não-governamentais (ONG ) que atuam na periferia , como a Afrobusiness e a Barca , que atendeu Maria Pimentel , da Parça Progresso Confecções , do Jardim Ângela , zona sul de São Paulo .',\n",
       "   \"É o que mostra o estudo publicado pelo britânico David Vaughan e seus colegas do British Antarctic Survey na revista 'Science' de esta semana.\",\n",
       "   'A startup de logística Pegaki cresceu dando aos clientes a opção de retirarem seus produtos em locais próximos à sua residência — uma estratégia que gigantes como a Magazine Luiza já praticam por meio da disseminação de suas lojas .'),\n",
       "  ('Xinhua',\n",
       "   'Airfox',\n",
       "   'C6',\n",
       "   'SeeTree',\n",
       "   'EDP',\n",
       "   'Folha de São Paulo',\n",
       "   'Google Adsense',\n",
       "   'Federação Nacional de Associações dos Servidores do Banco Central',\n",
       "   'Instituto Nacional do Envelhecimentos',\n",
       "   'Airfox',\n",
       "   'Via Varejo',\n",
       "   'Porto Seguro',\n",
       "   'Ruiz',\n",
       "   'Afrobusiness',\n",
       "   'British Antarctic Survey',\n",
       "   'Magazine Luiza'),\n",
       "  ('Partido Comunista Chinês',\n",
       "   ' Cielo',\n",
       "   ' MIT (Instituto o de Tecnologia de Massachusetts)',\n",
       "   'Brasil',\n",
       "   'Imprensa Nacional',\n",
       "   'AmBev',\n",
       "   'Neteller',\n",
       "   ' Brasília',\n",
       "   'Estados Unidos',\n",
       "   ' ZestFinance',\n",
       "   ' Airfox',\n",
       "   ' Brasil',\n",
       "   'PRI',\n",
       "   'Barca',\n",
       "   'Science',\n",
       "   'Pegaki'),\n",
       "  ('O N.R.F.C. sempre foi formador de grandes atletas do handebol disputando e obtendo títulos em campeonatos com a presença de grandes e tradicionais clubes de nosso Estado.',\n",
       "   'Uma infinidade de novas empresas de tecnologia financeira , incluindo as britânicas Revolut e Monzo , e a N26 , da Alemanha , seduziram milhões de clientes para suas plataformas .',\n",
       "   'O Rural Sustentável é uma parceria entre o Ministério da Agricultura , Pecuária e Abastecimento , o governo britânico , o Banco Interamericano de Desenvolvimento (BID) , tendo como implementador o Instituto Brasileiro de Desenvolvimento e Sustentabilidade (IABS) , com o apoio técnico da Empresa Brasileira de Pesquisa Agropecuária (Embrapa ) e do Banco do Brasil .',\n",
       "   'A Embrapa Café e o Consórcio Pesquisa Café , em parceria com a Universidade Federal de Lavras (UFLA) e a Universidade Federal de Viçosa (UFV) , selecionaram 26 startups para participarem da etapa piloto do projeto para a pré-aceleração de startups – Avança Café .',\n",
       "   'Com a plataforma ativa , 2019 será um ano de crescimento : ‘Temos convicção que 2019 será um ano de expressiva expansão’ , afirma João Nogueira , CEO da Appólice .',\n",
       "   'A iniciativa , da Money Cloud junto ao Instituto Banco da Periferia , começa com um piloto Maricá (RJ) , onde milhares de clientes já contam com serviços completos de conta digital , além de pagamentos nos pontos comerciais credenciados .',\n",
       "   'Efectivamente, penso que, uma vez que o nosso Parlamento, com muita oportunidade, convidou conjuntamente, há um mês, os Presidentes da Knesset e do Conselho Legislativo palestiniano, precisamente no sentido de afirmar a sua vontade de intervir como actor a favor do processo de paz, não podemos, enquanto Parlamento, calar nos hoje.',\n",
       "   'Desta vez o Museu lourinhanense está a usar Tomografia Axial Computorizada (TAC) para descobrir se existem mais ovos com embriões, os ossos dos dinossauros antes de Habitualmente usado para fins médicos, o equipamento de TAC foi disponibilizado pelas clínicas Cedima/IRM e Montepio das Caldas da Rainha em colaboração com os médicos Francisco Rita e José.',\n",
       "   'A Netshoes será incorporada por uma subsidiária do Magazine Luiza criada nas Ilhas Cayman , afirmou a rede de varejo .',\n",
       "   'A realização é fruto de uma parceria entre a companhia de biotecnologia Advanced Cell Technology, de Massachusetts, o Centro Sioux, de Iowa, e o Centro para Reprodução de Espécies Ameaçadas da Sociedade Zoológica de San Diego, na Califórnia.',\n",
       "   'O seguro cobre roubos e furtos , além de garantir a troca ou o conserto do aparelho em casos de quebra acidental – também válida para derramamento de líquidos (Imagem : Facebook Credicard ) A Credicard , empresa de soluções de pagamento do banco Itaú Unibanco (ITUB4) , acaba de anunciar sua parceria com a Chubb Seguros .',\n",
       "   \"A sua fundação, como 'Biblioteca de Marinha', deve-se à Rainha D. Maria II, por decreto de 7 de Janeiro de 1835, ficando de ínicio integrada no Arsenal Real de Marinha e instalada no andar nobre do respectivo edíficio da Ribeira das Naus.\",\n",
       "   \"Para Greg Laden, da Universidade de Minnesota, e Richard Wrangham, da Universidade Harvard, raízes, tubérculos e outros USOs (sigla em inglês para 'órgãos subterrâneos de armazenamento' das plantas) permitiram que os primeiros hominídeos, como são conhecidos os ancestrais do homem, saíssem da mata fechada e colonizassem ambientes abertos a partir de uns 6 milhões de anos atrás.\",\n",
       "   'Última alteração dia 2003-03-24 a as 08:57:03 Ovos de dinossauro vistos com o pormenor possível - O Museu da Lourinhã é conhecido pelos seus ovos e embriões de dinossauro, os únicos da Europa e dos mais antigos do Mundo.',\n",
       "   'O Banco Inter não é aceito por algumas empresas como o Google Adsense , Skrill e Neteller .',\n",
       "   'O orçamento da União, Senador Mendes Canale, é de mais de quatro trilhões de cruzeiros.'),\n",
       "  ('N.R.F.C.',\n",
       "   'Monzo',\n",
       "   'Rural Sustentável',\n",
       "   'Consórcio Pesquisa Café',\n",
       "   'CEO',\n",
       "   'Money Cloud',\n",
       "   'Parlamento',\n",
       "   'Montepio',\n",
       "   'Magazine Luiza',\n",
       "   'Advanced Cell Technology',\n",
       "   'Itaú Unibanco',\n",
       "   'Biblioteca de Marinha',\n",
       "   'Universidade Harvard',\n",
       "   'Museu da Lourinhã',\n",
       "   'Google Adsense',\n",
       "   'União'),\n",
       "  ('Estado',\n",
       "   'Alemanha',\n",
       "   ' Instituto Brasileiro de Desenvolvimento e Sustentabilidade (IABS',\n",
       "   'Universidade Federal de Viçosa',\n",
       "   'Appólice',\n",
       "   'Instituto Banco da Periferia',\n",
       "   'Knesset',\n",
       "   'Francisco Rita',\n",
       "   ' Ilhas Cayman',\n",
       "   'Massachusetts',\n",
       "   ' Chubb Seguros',\n",
       "   'Rainha D. Maria II',\n",
       "   'Richard Wrangham',\n",
       "   'Europa',\n",
       "   'Neteller',\n",
       "   'Senador Mendes Canale'),\n",
       "  ('Essa é a premissa do programa AgroUp/ Riap (Rd e Nacional de Inovação para a Agricultura e Pecuária) , lançado pela Famasul e Senar/MS na sexta-feira (4) , em Campo Grande .',\n",
       "   'E o ICANN assincou já o acordo em esse sentido com a mTLD Top Level Domain que congrega a GSM Association, a Microsoft, fabricantes como a Nokia, a Samsung e a Ericsson e operadores como a Vodafone, a Telefónica Móviles, a Hutchison 3 e a T-Mobile.',\n",
       "   'Mas isso não é verdade, argumenta Mark Wheelis da Universidade da California, Davis.',\n",
       "   'Preocupada em expandir a sua área de atuação e em tornar a experiência dentro de um banco mais agradável , a Havanna fecha parceria com o Santander para inaugurar um novo modelo de negócios .',\n",
       "   'Com 400 drones , a frota da Corteva Agriscience , divisão agrícola da DowDuPont , é a maior focada em agricultura do mundo – e agora usará a tecnologia da DroneDeploy , responsável por um sistema de mapeamento de alta precisão .',\n",
       "   'O trabalho foi coordenado pelo geocientista Charles Vörösmarty, da Universidade de New Hampshire, em os Estados Unidos.',\n",
       "   'A Via Varejo , dona das redes de lojas de eletrodomésticos Casas Bahia e Ponto Frio , vai disputar o mercado de serviços financeiros digitais .'),\n",
       "  ('Riap (Rd e Nacional de Inovação para a Agricultura e Pecuária',\n",
       "   'mTLD Top Level Domain',\n",
       "   'Mark Wheelis',\n",
       "   'Havanna',\n",
       "   'Corteva Agriscience',\n",
       "   'Universidade de New Hampshire',\n",
       "   'Via Varejo'),\n",
       "  (' Famasul',\n",
       "   'Vodafone',\n",
       "   'Universidade da California',\n",
       "   ' Santander',\n",
       "   ' DowDuPont',\n",
       "   'Estados Unidos',\n",
       "   'Casas Bahia')],\n",
       " tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
       " tensor([[ 3.6145, -4.0332],\n",
       "         [ 2.9454, -3.2850],\n",
       "         [ 3.6144, -4.1533],\n",
       "         [ 3.2951, -3.7534],\n",
       "         [ 2.6479, -3.2055],\n",
       "         [ 3.5210, -3.7361],\n",
       "         [ 3.5306, -3.8631],\n",
       "         [ 3.1539, -3.5439],\n",
       "         [ 3.1592, -3.4335],\n",
       "         [ 2.9470, -3.2517],\n",
       "         [ 3.4160, -3.8752],\n",
       "         [ 2.5030, -3.1347],\n",
       "         [ 3.6209, -3.8496],\n",
       "         [ 3.1145, -3.4090],\n",
       "         [ 3.5671, -4.0706],\n",
       "         [ 3.5630, -4.1705],\n",
       "         [ 3.5828, -4.1077],\n",
       "         [ 3.5591, -4.1522],\n",
       "         [ 2.8653, -3.3315],\n",
       "         [ 3.5051, -3.7830],\n",
       "         [ 2.9837, -3.6549],\n",
       "         [ 3.3669, -3.7401],\n",
       "         [ 3.6268, -3.9524],\n",
       "         [ 3.0783, -3.4830],\n",
       "         [ 3.3779, -3.7646],\n",
       "         [ 3.4863, -3.8825],\n",
       "         [ 2.7684, -3.1149],\n",
       "         [ 3.1355, -3.5830],\n",
       "         [ 3.4634, -3.9011],\n",
       "         [ 3.0659, -3.7228],\n",
       "         [ 3.5197, -4.0885],\n",
       "         [ 3.7711, -4.1912],\n",
       "         [ 3.3208, -3.7670],\n",
       "         [ 3.2015, -3.4017],\n",
       "         [ 3.7273, -4.1771],\n",
       "         [ 3.4072, -4.1054],\n",
       "         [ 3.5930, -3.9002],\n",
       "         [ 3.3070, -3.4933],\n",
       "         [ 3.3280, -3.6957],\n",
       "         [ 3.7596, -4.0982],\n",
       "         [ 3.3546, -3.7670],\n",
       "         [ 2.4842, -2.9920],\n",
       "         [ 3.7545, -4.2067],\n",
       "         [ 3.0918, -3.5973],\n",
       "         [ 3.7533, -3.9760],\n",
       "         [ 3.3374, -3.6355],\n",
       "         [ 3.6369, -4.1428],\n",
       "         [ 3.5160, -4.1071],\n",
       "         [ 3.7298, -4.2162],\n",
       "         [ 2.6445, -3.0310],\n",
       "         [ 3.2238, -3.4677],\n",
       "         [ 3.5540, -3.8435],\n",
       "         [ 3.6562, -3.9782],\n",
       "         [ 3.5520, -4.1140],\n",
       "         [ 3.6077, -4.0779],\n",
       "         [ 3.5057, -3.7925],\n",
       "         [ 3.6240, -4.0940],\n",
       "         [ 3.2407, -3.7134],\n",
       "         [ 3.7995, -4.0386],\n",
       "         [ 3.6120, -4.0669],\n",
       "         [ 2.8305, -3.3109],\n",
       "         [ 3.6349, -4.0665],\n",
       "         [ 3.4359, -3.9719],\n",
       "         [ 3.3138, -3.6828],\n",
       "         [ 2.8001, -3.3001],\n",
       "         [ 2.5889, -3.1799],\n",
       "         [ 3.5837, -4.1711],\n",
       "         [ 2.8353, -3.3827],\n",
       "         [ 2.7856, -3.3347],\n",
       "         [ 2.7264, -3.2243],\n",
       "         [ 3.4735, -3.7510],\n",
       "         [ 3.6608, -4.1429],\n",
       "         [ 3.3086, -3.8743],\n",
       "         [ 3.4782, -3.8951],\n",
       "         [ 2.6481, -3.1244],\n",
       "         [ 3.3075, -3.6040],\n",
       "         [ 3.7088, -4.0916],\n",
       "         [ 3.5532, -4.0008],\n",
       "         [ 2.4983, -2.9791],\n",
       "         [ 3.7423, -3.9866],\n",
       "         [ 3.2020, -3.7153],\n",
       "         [ 3.6025, -3.8985],\n",
       "         [ 2.5509, -3.1247],\n",
       "         [ 3.3012, -3.5448],\n",
       "         [ 3.5009, -3.9716],\n",
       "         [ 3.4305, -3.8025],\n",
       "         [ 3.6323, -4.0430],\n",
       "         [ 3.0654, -3.6311],\n",
       "         [ 3.3566, -3.6651],\n",
       "         [ 3.4632, -4.0143],\n",
       "         [ 3.5783, -4.0811],\n",
       "         [ 2.8870, -3.3853],\n",
       "         [ 3.7587, -4.2093],\n",
       "         [ 3.6836, -4.0913],\n",
       "         [ 3.7516, -4.0753],\n",
       "         [ 3.3628, -3.7922],\n",
       "         [ 3.2692, -3.7488],\n",
       "         [ 3.4545, -4.0216],\n",
       "         [ 3.3993, -3.9774],\n",
       "         [ 3.3466, -4.0554],\n",
       "         [ 3.1654, -3.6538],\n",
       "         [ 3.1488, -3.7173],\n",
       "         [ 3.2476, -3.7700],\n",
       "         [ 3.5121, -4.0278],\n",
       "         [ 3.4791, -3.9065],\n",
       "         [ 3.3002, -3.7667],\n",
       "         [ 3.1270, -3.7609],\n",
       "         [ 3.3474, -3.5978],\n",
       "         [ 3.0965, -3.8100],\n",
       "         [ 3.4198, -3.8600],\n",
       "         [ 3.4895, -3.9842],\n",
       "         [ 3.6247, -4.1962],\n",
       "         [ 3.3726, -3.9552],\n",
       "         [ 2.7340, -3.2346],\n",
       "         [ 3.4038, -3.8118],\n",
       "         [ 3.6509, -4.2835],\n",
       "         [ 3.4882, -4.0182],\n",
       "         [ 3.1959, -3.7705],\n",
       "         [ 2.8528, -3.2713],\n",
       "         [ 3.4667, -4.0117],\n",
       "         [ 3.4178, -3.9557],\n",
       "         [ 3.5849, -4.1676],\n",
       "         [ 3.4914, -4.0582],\n",
       "         [ 2.8077, -3.2823],\n",
       "         [ 3.5022, -4.1025],\n",
       "         [ 3.6637, -4.0176],\n",
       "         [ 3.4521, -3.8460],\n",
       "         [ 3.5158, -4.0065],\n",
       "         [ 3.5095, -4.1737],\n",
       "         [ 3.0354, -3.6309],\n",
       "         [ 3.2522, -3.7258],\n",
       "         [ 2.8402, -3.3596],\n",
       "         [ 3.6401, -4.0693],\n",
       "         [ 3.2081, -3.9021],\n",
       "         [ 3.2608, -3.7414],\n",
       "         [ 3.7481, -4.1990],\n",
       "         [ 3.2216, -3.7645],\n",
       "         [ 3.4866, -3.9791],\n",
       "         [ 2.9797, -3.3106],\n",
       "         [ 3.5473, -3.9545],\n",
       "         [ 3.1644, -3.6337],\n",
       "         [ 3.4781, -4.0271],\n",
       "         [ 3.6109, -4.0437],\n",
       "         [ 3.6708, -4.1323],\n",
       "         [ 3.6960, -4.1239],\n",
       "         [ 2.7438, -3.3371],\n",
       "         [ 3.2176, -3.7052],\n",
       "         [ 3.6820, -4.1047],\n",
       "         [ 3.5872, -4.0721],\n",
       "         [ 3.1521, -3.6239],\n",
       "         [ 3.1057, -3.4927],\n",
       "         [ 2.4060, -2.7493],\n",
       "         [ 3.6409, -4.1366],\n",
       "         [ 3.6005, -4.1400],\n",
       "         [ 3.3221, -3.7853],\n",
       "         [ 3.2050, -3.4254],\n",
       "         [ 3.7593, -3.9838],\n",
       "         [ 2.6565, -3.0270],\n",
       "         [ 3.5277, -3.8078],\n",
       "         [ 3.5238, -3.7475],\n",
       "         [ 3.5187, -4.0967],\n",
       "         [ 3.5784, -4.1685],\n",
       "         [ 3.4557, -3.9445],\n",
       "         [ 3.4545, -4.0440],\n",
       "         [ 3.7059, -4.1221],\n",
       "         [ 3.1839, -3.7492],\n",
       "         [ 3.4904, -3.7023],\n",
       "         [ 3.6634, -4.2457],\n",
       "         [ 3.4901, -3.9402],\n",
       "         [ 3.5129, -4.0123],\n",
       "         [ 3.4982, -3.9798],\n",
       "         [ 3.3021, -3.6676],\n",
       "         [ 3.2407, -3.7134],\n",
       "         [ 3.6587, -4.2926],\n",
       "         [ 3.4899, -4.0679],\n",
       "         [ 3.6510, -4.0618],\n",
       "         [ 3.0385, -3.4006],\n",
       "         [ 3.7025, -4.1093],\n",
       "         [ 3.6270, -4.1717],\n",
       "         [ 3.4545, -4.0216],\n",
       "         [ 3.1263, -3.6353],\n",
       "         [ 3.5943, -3.8997],\n",
       "         [ 3.2053, -3.7341],\n",
       "         [ 3.8064, -4.0493],\n",
       "         [ 3.2181, -3.6555],\n",
       "         [ 3.5604, -4.0754],\n",
       "         [ 3.9961, -4.2565],\n",
       "         [ 3.5730, -4.1688],\n",
       "         [ 2.9668, -3.4246],\n",
       "         [ 3.3200, -3.9182],\n",
       "         [ 3.0718, -3.5080],\n",
       "         [ 3.7629, -4.0337],\n",
       "         [ 2.4391, -2.7889],\n",
       "         [ 3.6635, -4.1363],\n",
       "         [ 2.9940, -3.5802],\n",
       "         [ 3.4765, -3.9927],\n",
       "         [ 3.5524, -3.9236],\n",
       "         [ 3.6144, -4.1898],\n",
       "         [ 2.9130, -3.3070],\n",
       "         [ 3.0014, -3.5502],\n",
       "         [ 3.6138, -4.0903],\n",
       "         [ 3.1864, -3.7497],\n",
       "         [ 3.3182, -3.9451],\n",
       "         [ 2.9676, -3.2796],\n",
       "         [ 3.4185, -3.8267],\n",
       "         [ 2.8707, -3.4277],\n",
       "         [ 3.2053, -3.7341],\n",
       "         [ 2.7251, -3.1740],\n",
       "         [ 3.3057, -3.7883],\n",
       "         [ 3.6002, -4.0642],\n",
       "         [ 2.7866, -3.3922],\n",
       "         [ 3.5214, -4.1373],\n",
       "         [ 3.5812, -4.1916],\n",
       "         [ 3.1576, -3.6407],\n",
       "         [ 3.5951, -4.0012]]),\n",
       " tensor([0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1,\n",
       "         1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1,\n",
       "         1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0,\n",
       "         1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0,\n",
       "         0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1,\n",
       "         1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1,\n",
       "         1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1,\n",
       "         1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0,\n",
       "         0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1]))"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_review_texts, y_pred, y_pred_probs, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
